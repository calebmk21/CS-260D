{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1XuGLy_VOFfCZUuYph5oUVG_e100HAHhb","timestamp":1762919989525}],"collapsed_sections":["-3picnfHJVeL","moU18r8FJb5R","zAmNVVXiJk3V"],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Initial Set-up:\n","This section installs and imports any required libraries that will be used in the code below:"],"metadata":{"id":"vbc3jTbvIR3l"}},{"cell_type":"code","source":["!pip install opacus\n","!pip install apricot-select numpy scikit-learn\n","!pip install kmedoids"],"metadata":{"collapsed":true,"id":"wGgRT0_uINbB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764491672427,"user_tz":480,"elapsed":12521,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"9d8d0fb8-8b77-49bd-9683-a613eafe2540"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opacus in /usr/local/lib/python3.12/dist-packages (1.5.4)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.12/dist-packages (from opacus) (2.0.2)\n","Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from opacus) (2.9.0+cu126)\n","Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.12/dist-packages (from opacus) (1.16.3)\n","Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from opacus) (3.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.5.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->opacus) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.3)\n","Requirement already satisfied: apricot-select in /usr/local/lib/python3.12/dist-packages (0.6.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from apricot-select) (1.16.3)\n","Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.12/dist-packages (from apricot-select) (0.60.0)\n","Requirement already satisfied: tqdm>=4.24.0 in /usr/local/lib/python3.12/dist-packages (from apricot-select) (4.67.1)\n","Requirement already satisfied: nose in /usr/local/lib/python3.12/dist-packages (from apricot-select) (1.3.7)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.43.0->apricot-select) (0.43.0)\n","Requirement already satisfied: kmedoids in /usr/local/lib/python3.12/dist-packages (0.5.4)\n"]}]},{"cell_type":"code","source":["# For the CNN + DP-SGD\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, random_split, Subset\n","import numpy as np\n","from opacus import PrivacyEngine\n","import time\n","import matplotlib.pyplot as plt\n","\n","#For Lazy Greedy\n","from sklearn.metrics import pairwise_distances\n","import heapq\n","from sklearn.metrics import pairwise_distances\n","from apricot import FacilityLocationSelection\n","import time\n","\n","#For K-medoids\n","import kmedoids\n","from sklearn.metrics.pairwise import euclidean_distances"],"metadata":{"id":"_20F5IwnIxrp","executionInfo":{"status":"ok","timestamp":1764491679357,"user_tz":480,"elapsed":6922,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Select Dataset\n","All dataset arguments default to MNIST. This goes for the `run_experiment` function later. It is important to run `select_dataset` at least once as this loads the data. If you opt to run all cells, it will load MNIST by default."],"metadata":{"id":"7xlsjHbHJAwp"}},{"cell_type":"code","source":["def select_dataset_greedy(name, n_samples):\n","  if name == \"MNIST\":\n","    transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,)),])\n","\n","    full_train = datasets.MNIST(\n","        root=\"./data\",\n","        train=True,\n","        download=True,\n","        transform=transform\n","    )\n","\n","    train_size = 10000\n","    #val_size = 1000\n","    val_size = len(full_train) - train_size\n","    # train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n","    train_dataset, subset_rest = random_split(full_train, [train_size, len(full_train) - train_size])\n","    val_dataset, _ = random_split(subset_rest, [val_size, len(subset_rest) - val_size])\n","\n","    test_dataset = datasets.MNIST(\n","        root=\"./data\",\n","        train=False,\n","        download=True,\n","        transform=transform\n","    )\n","\n","  elif name == \"CIFAR\":\n","    transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n","\n","\n","    full_train = datasets.CIFAR10(\n","        root=\"./data\",\n","        train=True,\n","        download=True,\n","        transform=transform\n","    )\n","\n","    train_size = 10000\n","    val_size = len(full_train) - train_size\n","    #train_size = 5000\n","    #val_size = len(full_train) - train_size\n","    #val_size = 1000\n","    # train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n","    train_dataset, subset_rest = random_split(full_train, [train_size, len(full_train) - train_size])\n","    val_dataset, _ = random_split(subset_rest, [val_size, len(subset_rest) - val_size])\n","\n","\n","    test_dataset = datasets.CIFAR10(\n","        root=\"./data\",\n","        train=False,\n","        download=True,\n","        transform=transform\n","    )\n","\n","  # train_data_array = train_dataset.dataset.data[train_dataset.indices]\n","  # CIFAR10/100: data is already numpy array (shape [N, 32, 32, 3])\n","  # train_data_array = train_dataset.dataset.data[train_dataset.indices]\n","  train_data_array = train_dataset.dataset.data[train_dataset.indices]\n","  # Convert to numpy array first if it's a tensor, then convert type\n","  if isinstance(train_data_array, torch.Tensor):\n","    train_data_array = train_data_array.cpu().numpy()\n","  #train_data_array = train_data_array.astype('float32')\n","\n","  #----------------------MADI ADDED BECAUSE RAM SUCKS---------------------------\n","\n","  # Create a smaller subset for FacilityLocationSelection to prevent crashes due to large memory usage\n","  # Adjusting the subset size for testing, e.g., 1000 samples\n","  # subset_size_for_selection = min(1000, train_data_array.shape[0])\n","  # # Randomly sample indices for the subset\n","  # np.random.seed(SEED) # Ensure reproducibility\n","  # random_indices = np.random.choice(train_data_array.shape[0], subset_size_for_selection, replace=False)\n","  # subset_for_selection = train_data_array[random_indices]\n","\n","  #-----------------------------------------------------------------------------\n","\n","  selector = FacilityLocationSelection(n_samples=n_samples, metric='euclidean', optimizer='lazy', verbose=True)\n","  #selector = FacilityLocationSelection(n_samples=n_samples, optimizer='lazy', verbose=True)\n","\n","  # Reshape the subset for selection\n","  #subset_for_selection = subset_for_selection.reshape(subset_for_selection.shape[0], -1)\n","  train_data_array = train_data_array.reshape(train_data_array.shape[0], -1)\n","\n","  # Use the subset for selection\n","  #selector.fit(subset_for_selection)\n","  selector.fit(train_data_array)\n","\n","  selected_indices = selector.ranking\n","\n","  selected_train_dataset = Subset(train_dataset, selected_indices)\n","\n","  train_loader = DataLoader(selected_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","  val_loader   = DataLoader(val_dataset, batch_size=256, shuffle=False)\n","  test_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False)\n","\n","  return train_loader, val_loader, test_loader"],"metadata":{"id":"LE0RopAmJCQP","executionInfo":{"status":"ok","timestamp":1764491679361,"user_tz":480,"elapsed":3,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def select_dataset_kmedoids(name, n_clusters):\n","  if name == \"MNIST\":\n","    transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,)),])\n","\n","    full_train = datasets.MNIST(\n","        root=\"./data\",\n","        train=True,\n","        download=True,\n","        transform=transform\n","    )\n","\n","    train_size = 50000\n","    #val_size = 1000\n","    val_size = len(full_train) - train_size\n","    train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n","\n","    test_dataset = datasets.MNIST(\n","        root=\"./data\",\n","        train=False,\n","        download=True,\n","        transform=transform\n","    )\n","\n","  elif name == \"CIFAR\":\n","    transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n","\n","\n","    full_train = datasets.CIFAR10(\n","        root=\"./data\",\n","        train=True,\n","        download=True,\n","        transform=transform\n","    )\n","\n","    #train_size = 5000\n","    #val_size = 1000\n","    train_size = 50000\n","    val_size = len(full_train) - train_size\n","    train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n","\n","\n","    test_dataset = datasets.CIFAR10(\n","        root=\"./data\",\n","        train=False,\n","        download=True,\n","        transform=transform\n","    )\n","\n","  train_data_array = train_dataset.dataset.data[train_dataset.indices]\n","  # Convert to numpy array first if it's a tensor, then convert type\n","  if isinstance(train_data_array, torch.Tensor):\n","    train_data_array = train_data_array.cpu().numpy()\n","  train_data_array = train_data_array.astype('float32')\n","  train_data_flat = train_data_array.reshape(train_data_array.shape[0], -1)\n","\n","  # Create a smaller subset for kmedoids to prevent crashes due to large memory usage\n","  subset_size_for_kmedoids = min(5000, train_data_array.shape[0]) # Adjusted subset size\n","  np.random.seed(SEED) # Ensure reproducibility\n","  random_indices_kmedoids = np.random.choice(train_data_array.shape[0], subset_size_for_kmedoids, replace=False)\n","  subset_for_kmedoids = train_data_flat[random_indices_kmedoids]\n","\n","  # Calculate distances only for the subset\n","  diss = euclidean_distances(subset_for_kmedoids)\n","\n","  kmeds = kmedoids.KMedoids(n_clusters=n_clusters, method='fasterpam', random_state=0, metric='precomputed')\n","  model = kmeds.fit(diss)\n","\n","  # Map the medoid indices from the subset back to the original dataset indices\n","  selected_subset_indices = model.medoid_indices_\n","  selected_original_indices = random_indices_kmedoids[selected_subset_indices]\n","  select = [train_dataset.indices[i] for i in selected_original_indices]\n","\n","  selected_train_dataset = Subset(full_train, select)\n","\n","  train_loader = DataLoader(selected_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","  val_loader   = DataLoader(val_dataset, batch_size=256, shuffle=False)\n","  test_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False)\n","\n","  return train_loader, val_loader, test_loader"],"metadata":{"id":"miRy9BYMJLbq","executionInfo":{"status":"ok","timestamp":1764492199817,"user_tz":480,"elapsed":7,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["# Baseline CNN Model\n","\n","This model is chosen in `run_experiment` if an invalid dataset is selected. I should probably replace this with a `ThrowException` instead though."],"metadata":{"id":"NyIPeIWlJPow"}},{"cell_type":"code","source":["# baseline model\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        #=======================================#\n","        #            MNIST Settings\n","        #=======================================#\n","\n","        # self.conv1 = nn.Conv2d(1, 16, 3, 1)\n","        # self.conv2 = nn.Conv2d(16, 32, 3, 1)\n","        # self.fc1 = nn.Linear(32*12*12, 64)\n","        # self.fc2 = nn.Linear(64, 10)\n","\n","        #=======================================#\n","        #            CIFAR-10 Settings\n","        #=======================================#\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)\n","        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.fc1 = nn.Linear(in_features=576, out_features=64)\n","        self.fc2 = nn.Linear(in_features=64, out_features=10)\n","\n","    def forward(self, x):\n","        # x = F.relu(self.conv1(x))\n","        # x = F.relu(self.conv2(x))\n","        # x = F.max_pool2d(x, 2)\n","        # x = torch.flatten(x, 1)\n","        # x = F.relu(self.fc1(x))\n","        # return self.fc2(x)\n","\n","        x = self.pool(torch.relu(self.conv1(x)))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1)\n","\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n"],"metadata":{"id":"eIp2H2PeJTFN","executionInfo":{"status":"ok","timestamp":1764492201936,"user_tz":480,"elapsed":16,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["## Model Architectures\n","MNIST and CIFAR-10 require different NN structures, so they have their own separate classes that get initialized in `run_experiment`."],"metadata":{"id":"-3picnfHJVeL"}},{"cell_type":"code","source":["# MNIST Model\n","class CNN_MNIST(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        #=======================================#\n","        #            MNIST Settings\n","        #=======================================#\n","\n","        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n","        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n","        self.fc1 = nn.Linear(32*12*12, 64)\n","        self.fc2 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2)\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        return self.fc2(x)\n","\n","\n","# CIFAR-10 Model\n","\n","class CNN_CIFAR10(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        #=======================================#\n","        #            CIFAR-10 Settings\n","        #=======================================#\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)\n","        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.fc1 = nn.Linear(in_features=576, out_features=64)\n","        self.fc2 = nn.Linear(in_features=64, out_features=10)\n","\n","    def forward(self, x):\n","\n","        x = self.pool(torch.relu(self.conv1(x)))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1)\n","\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n"],"metadata":{"id":"G4kxKosOJZvP","executionInfo":{"status":"ok","timestamp":1764492203393,"user_tz":480,"elapsed":13,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["# Model Training\n","Courtesy of Megha"],"metadata":{"id":"moU18r8FJb5R"}},{"cell_type":"code","source":["# @title\n","def train_one_epoch(model, loader, optimizer):\n","    # train loop\n","    model.train()\n","    total_loss = 0\n","    for batch_idx, (data, target) in enumerate(loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        out = model(data)\n","        loss = F.cross_entropy(out, target)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(loader)"],"metadata":{"id":"7JgsYVwaJcn0","executionInfo":{"status":"ok","timestamp":1764492205074,"user_tz":480,"elapsed":8,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"cellView":"form"},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# @title\n","@torch.no_grad()\n","def evaluate(model, loader):\n","    # evaluation\n","    model.eval()\n","    loss, correct = 0, 0\n","    for data, target in loader:\n","        data, target = data.to(device), target.to(device)\n","        out = model(data)\n","        loss += F.cross_entropy(out, target, reduction=\"sum\").item()\n","        pred = out.argmax(1)\n","        correct += pred.eq(target).sum().item()\n","    loss /= len(loader.dataset)\n","    acc = 100. * correct / len(loader.dataset)\n","    return loss, acc"],"metadata":{"id":"AhnJwpmuJfPg","executionInfo":{"status":"ok","timestamp":1764492205699,"user_tz":480,"elapsed":3,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"cellView":"form"},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# @title\n","\"\"\"\n","dataset: either MNIST or CIFAR\n","I set dataset_name as a hyperparameter we can use for convenience and easy swapping\n","Otherwise, it defaults to MNIST, and invalid datasets give the base CNN model\n","This makes the dataset selection a modular component, should we want to choose other datasets\n","\"\"\"\n","\n","def run_experiment(dataset, use_dp=False, fixed_privacy_budget=False):\n","\n","    # setup model and optimizer\n","    if dataset == 'MNIST':\n","      model = CNN_MNIST().to(device)\n","    elif dataset == \"CIFAR\":\n","      model = CNN_CIFAR10().to(device)\n","\n","    # Defaults to base CNN class if no dataset is specified\n","    else:\n","      model = CNN().to(device)\n","\n","    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n","\n","    privacy_engine = None\n","    if use_dp:\n","\n","        if fixed_privacy_budget:\n","\n","            privacy_engine = PrivacyEngine()\n","\n","            # Calculates sigma based on target epsilon and delta\n","            model, optimizer, train_loader_dp = privacy_engine.make_private_with_epsilon(\n","              module=model,\n","              optimizer=optimizer,\n","              data_loader=train_loader,\n","              max_grad_norm=MAX_GRAD_NORM,\n","              target_delta=TARGET_DELTA,\n","              target_epsilon=TARGET_EPSILON,\n","              epochs=NUM_EPOCHS\n","        )\n","        else:\n","\n","            privacy_engine = PrivacyEngine()\n","\n","            # Otherwise takes sigma as a hyperparameter\n","            model, optimizer, train_loader_dp = privacy_engine.make_private(\n","              module=model,\n","              optimizer=optimizer,\n","              data_loader=train_loader,\n","              max_grad_norm=MAX_GRAD_NORM,\n","              noise_multiplier=NOISE_MULTIPLIER,\n","        )\n","    else:\n","        train_loader_dp = train_loader\n","\n","\n","    # train\n","    best_val_acc = 0\n","    epochs_no_improve = 0\n","    best_model_path = f\"best_model{'_dp' if use_dp else ''}.pt\"\n","\n","    start_time = time.time()\n","\n","    for epoch in range(1, EPOCHS + 1):\n","        train_loss = train_one_epoch(model, train_loader_dp, optimizer)\n","        val_loss, val_acc = evaluate(model, val_loader)\n","\n","        eps = privacy_engine.get_epsilon(DELTA)\n","\n","        print(f\"[{'DP-SGD' if use_dp else 'Standard SGD'}] Epoch {epoch}: \"\n","            + f\"train_loss={train_loss:.4f}, \"\n","            + f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}%\"\n","            + f\", ε={eps:.4f}\" if use_dp else \"\")\n","\n","        # if fixed_privacy_budget:\n","        #   eps = privacy_engine.get_epsilon(TARGET_DELTA)\n","        #   print(f\", ε={eps:.4f}\")\n","\n","        # elif use_dp:\n","        #   eps = privacy_engine.get_epsilon(DELTA)\n","        #   print()\n","\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            epochs_no_improve = 0\n","            torch.save(model.state_dict(), best_model_path)\n","        else:\n","            epochs_no_improve += 1\n","\n","        if epochs_no_improve >= PATIENCE:\n","            print(f\"\\nEarly stopping at epoch {epoch} (no improvement for {PATIENCE} epochs).\")\n","            break\n","\n","\n","    end_time = time.time()\n","    total_time = end_time - start_time\n","\n","    # test on best model\n","    model.load_state_dict(torch.load(best_model_path))\n","    test_loss, test_acc = evaluate(model, test_loader)\n","    final_eps = privacy_engine.get_epsilon(DELTA) if use_dp else None\n","\n","    print(f\"\\n[{ 'DP-SGD' if use_dp else 'Standard-SGD'}]\")\n","    print(f\"Best val acc: {best_val_acc:.4f}%, Test acc: {test_acc:.4f}%\\n\" + (f\" ε={final_eps:.4f}\\n\" if use_dp else \"\") + (f\"Total runtime: {total_time:.4f} seconds\"))\n","    return best_val_acc, test_acc, final_eps"],"metadata":{"id":"r1paKWESJh7n","executionInfo":{"status":"ok","timestamp":1764492206227,"user_tz":480,"elapsed":12,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"cellView":"form"},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["# Initial Benchmarks\n","## Model Hyperparameters"],"metadata":{"id":"zAmNVVXiJk3V"}},{"cell_type":"code","source":["# @title\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","BATCH_SIZE = 128\n","LR = 0.01\n","EPOCHS = 20\n","SEED = 42\n","MAX_GRAD_NORM = 1.0\n","DELTA = 1e-5\n","PATIENCE = 5\n","\n","# For dynamically calculated epsilon\n","NOISE_MULTIPLIER = 1.0\n","\n","# For a fixed privacy budget (eps, del)\n","TARGET_EPSILON = 8\n","TARGET_DELTA = 1e-5\n","NUM_EPOCHS = 20\n","\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)"],"metadata":{"id":"kBXzBBNvJlja","executionInfo":{"status":"ok","timestamp":1764492207029,"user_tz":480,"elapsed":4,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"cellView":"form"},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["# TESTS:"],"metadata":{"id":"T2hlRjQ7Jt8F"}},{"cell_type":"markdown","source":["## MNIST"],"metadata":{"id":"nrUn6qDmJx1H"}},{"cell_type":"markdown","source":["### Lazy Greedy"],"metadata":{"id":"pTdS9vRvJ0R2"}},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_SAMPLES = 100\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dLR_m83bKAZu","executionInfo":{"status":"ok","timestamp":1764460754868,"user_tz":480,"elapsed":28835,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"94c77bb7-6c09-483a-f514-7ea50b3e683d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 37.5MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 1.04MB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 9.38MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 9.57MB/s]\n","100%|██████████| 100/100 [00:00<00:00, 171it/s] \n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:67: RuntimeWarning: divide by zero encountered in log\n","  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:70: RuntimeWarning: divide by zero encountered in log\n","  t > np.log(1 - q),\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1715503071.py:10: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n","  loss.backward()\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3196, val_loss=2.3154, val_acc=7.3000%, ε=1.4451\n","[DP-SGD] Epoch 2: train_loss=2.3182, val_loss=2.3134, val_acc=8.1000%, ε=2.1200\n","[DP-SGD] Epoch 3: train_loss=2.3153, val_loss=2.3105, val_acc=9.2000%, ε=2.6597\n","[DP-SGD] Epoch 4: train_loss=2.3108, val_loss=2.3071, val_acc=10.3000%, ε=3.1286\n","[DP-SGD] Epoch 5: train_loss=2.3052, val_loss=2.3039, val_acc=11.1000%, ε=3.5518\n","[DP-SGD] Epoch 6: train_loss=2.2995, val_loss=2.3005, val_acc=12.0000%, ε=3.9423\n","[DP-SGD] Epoch 7: train_loss=2.2936, val_loss=2.2973, val_acc=14.1000%, ε=4.3079\n","[DP-SGD] Epoch 8: train_loss=2.2879, val_loss=2.2938, val_acc=15.6000%, ε=4.6538\n","[DP-SGD] Epoch 9: train_loss=2.2818, val_loss=2.2899, val_acc=16.5000%, ε=4.9834\n","[DP-SGD] Epoch 10: train_loss=2.2751, val_loss=2.2857, val_acc=17.6000%, ε=5.2994\n","[DP-SGD] Epoch 11: train_loss=2.2682, val_loss=2.2815, val_acc=18.1000%, ε=5.6037\n","[DP-SGD] Epoch 12: train_loss=2.2609, val_loss=2.2768, val_acc=18.4000%, ε=5.8979\n","[DP-SGD] Epoch 13: train_loss=2.2530, val_loss=2.2721, val_acc=18.1000%, ε=6.1833\n","[DP-SGD] Epoch 14: train_loss=2.2454, val_loss=2.2674, val_acc=18.1000%, ε=6.4607\n","[DP-SGD] Epoch 15: train_loss=2.2376, val_loss=2.2627, val_acc=17.9000%, ε=6.7310\n","[DP-SGD] Epoch 16: train_loss=2.2296, val_loss=2.2579, val_acc=18.3000%, ε=6.9949\n","[DP-SGD] Epoch 17: train_loss=2.2216, val_loss=2.2525, val_acc=19.1000%, ε=7.2530\n","[DP-SGD] Epoch 18: train_loss=2.2123, val_loss=2.2466, val_acc=19.7000%, ε=7.5058\n","[DP-SGD] Epoch 19: train_loss=2.2024, val_loss=2.2410, val_acc=20.5000%, ε=7.7537\n","[DP-SGD] Epoch 20: train_loss=2.1928, val_loss=2.2351, val_acc=22.1000%, ε=7.9971\n","\n","[DP-SGD]\n","Best val acc: 22.1000%, Test acc: 22.4800%\n"," ε=7.9971\n","Total runtime: 17.4841 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(22.1, 22.48, np.float64(7.997143272375973))"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_SAMPLES = 200\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LDwnuFQHKI_4","executionInfo":{"status":"ok","timestamp":1764460920516,"user_tz":480,"elapsed":57566,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"bb6571f7-8d53-4dfa-d1f9-63bc4bb2c74d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [00:00<00:00, 265it/s]\n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","/tmp/ipython-input-1715503071.py:10: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n","  loss.backward()\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3105, val_loss=2.2990, val_acc=10.2000%, ε=1.7625\n","[DP-SGD] Epoch 2: train_loss=2.3029, val_loss=2.2933, val_acc=11.7000%, ε=2.4240\n","[DP-SGD] Epoch 3: train_loss=2.2951, val_loss=2.2852, val_acc=15.1000%, ε=2.9456\n","[DP-SGD] Epoch 4: train_loss=2.2857, val_loss=2.2757, val_acc=18.2000%, ε=3.3954\n","[DP-SGD] Epoch 5: train_loss=2.2692, val_loss=2.2657, val_acc=21.4000%, ε=3.7996\n","[DP-SGD] Epoch 6: train_loss=2.2621, val_loss=2.2552, val_acc=24.1000%, ε=4.1714\n","[DP-SGD] Epoch 7: train_loss=2.2440, val_loss=2.2443, val_acc=26.4000%, ε=4.5186\n","[DP-SGD] Epoch 8: train_loss=2.2292, val_loss=2.2334, val_acc=30.1000%, ε=4.8463\n","[DP-SGD] Epoch 9: train_loss=2.1994, val_loss=2.2216, val_acc=31.1000%, ε=5.1581\n","[DP-SGD] Epoch 10: train_loss=2.1959, val_loss=2.2085, val_acc=33.0000%, ε=5.4565\n","[DP-SGD] Epoch 11: train_loss=2.1756, val_loss=2.1948, val_acc=35.4000%, ε=5.7435\n","[DP-SGD] Epoch 12: train_loss=2.1528, val_loss=2.1804, val_acc=36.2000%, ε=6.0207\n","[DP-SGD] Epoch 13: train_loss=2.1570, val_loss=2.1659, val_acc=36.7000%, ε=6.2892\n","[DP-SGD] Epoch 14: train_loss=2.1166, val_loss=2.1518, val_acc=37.3000%, ε=6.5499\n","[DP-SGD] Epoch 15: train_loss=2.0991, val_loss=2.1366, val_acc=38.4000%, ε=6.8038\n","[DP-SGD] Epoch 16: train_loss=2.0630, val_loss=2.1190, val_acc=39.0000%, ε=7.0515\n","[DP-SGD] Epoch 17: train_loss=2.0433, val_loss=2.0997, val_acc=39.5000%, ε=7.2935\n","[DP-SGD] Epoch 18: train_loss=1.9937, val_loss=2.0797, val_acc=40.0000%, ε=7.5303\n","[DP-SGD] Epoch 19: train_loss=1.9988, val_loss=2.0580, val_acc=40.5000%, ε=7.7625\n","[DP-SGD] Epoch 20: train_loss=1.9606, val_loss=2.0331, val_acc=41.9000%, ε=7.9902\n","\n","[DP-SGD]\n","Best val acc: 41.9000%, Test acc: 40.9500%\n"," ε=7.9902\n","Total runtime: 34.9375 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(41.9, 40.95, np.float64(7.990183309027119))"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_SAMPLES = 300\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugaxcy1qKM_R","executionInfo":{"status":"ok","timestamp":1764491483628,"user_tz":480,"elapsed":125900,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"ec34bd41-7a08-4048-bcf0-0f7baa3903f2"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 300/300 [00:32<00:00, 9.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3054, val_loss=2.3046, val_acc=10.2100%, ε=1.9068\n","[DP-SGD] Epoch 2: train_loss=2.2925, val_loss=2.3029, val_acc=11.0200%, ε=2.5514\n","[DP-SGD] Epoch 3: train_loss=2.2932, val_loss=2.3007, val_acc=11.9300%, ε=3.0612\n","[DP-SGD] Epoch 4: train_loss=2.2984, val_loss=2.2980, val_acc=13.2400%, ε=3.5012\n","[DP-SGD] Epoch 5: train_loss=2.2956, val_loss=2.2951, val_acc=15.2100%, ε=3.8968\n","[DP-SGD] Epoch 6: train_loss=2.2873, val_loss=2.2916, val_acc=16.7700%, ε=4.2607\n","[DP-SGD] Epoch 7: train_loss=2.2800, val_loss=2.2875, val_acc=16.7800%, ε=4.6005\n","[DP-SGD] Epoch 8: train_loss=2.2735, val_loss=2.2831, val_acc=16.2800%, ε=4.9213\n","[DP-SGD] Epoch 9: train_loss=2.2623, val_loss=2.2785, val_acc=16.2900%, ε=5.2264\n","[DP-SGD] Epoch 10: train_loss=2.2602, val_loss=2.2737, val_acc=15.6400%, ε=5.5184\n","[DP-SGD] Epoch 11: train_loss=2.2545, val_loss=2.2693, val_acc=14.9200%, ε=5.7993\n","[DP-SGD] Epoch 12: train_loss=2.2438, val_loss=2.2646, val_acc=14.2600%, ε=6.0705\n","\n","Early stopping at epoch 12 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 16.7800%, Test acc: 22.5800%\n"," ε=6.0705\n","Total runtime: 35.4687 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(16.78, 22.58, np.float64(6.070487622310106))"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_SAMPLES = 500\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IPa8cF9J1ryh","executionInfo":{"status":"ok","timestamp":1764491959637,"user_tz":480,"elapsed":246834,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"360cbe56-5698-41c9-c45a-d79f357f7735"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [00:02<00:00, 213it/s]\n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3066, val_loss=2.2973, val_acc=10.5820%, ε=1.9909\n","[DP-SGD] Epoch 2: train_loss=2.2871, val_loss=2.2859, val_acc=11.0320%, ε=2.6240\n","[DP-SGD] Epoch 3: train_loss=2.2733, val_loss=2.2734, val_acc=10.6120%, ε=3.1260\n","[DP-SGD] Epoch 4: train_loss=2.2504, val_loss=2.2607, val_acc=9.8840%, ε=3.5598\n","[DP-SGD] Epoch 5: train_loss=2.2303, val_loss=2.2467, val_acc=9.8160%, ε=3.9499\n","[DP-SGD] Epoch 6: train_loss=2.2086, val_loss=2.2286, val_acc=12.4840%, ε=4.3089\n","[DP-SGD] Epoch 7: train_loss=2.1845, val_loss=2.2082, val_acc=19.3300%, ε=4.6443\n","[DP-SGD] Epoch 8: train_loss=2.1536, val_loss=2.1845, val_acc=24.6360%, ε=4.9609\n","[DP-SGD] Epoch 9: train_loss=2.1149, val_loss=2.1568, val_acc=32.4360%, ε=5.2620\n","[DP-SGD] Epoch 10: train_loss=2.0778, val_loss=2.1238, val_acc=41.7440%, ε=5.5503\n","[DP-SGD] Epoch 11: train_loss=2.0503, val_loss=2.0877, val_acc=46.2000%, ε=5.8275\n","[DP-SGD] Epoch 12: train_loss=2.0034, val_loss=2.0454, val_acc=48.0040%, ε=6.0951\n","[DP-SGD] Epoch 13: train_loss=1.9205, val_loss=1.9949, val_acc=49.2220%, ε=6.3544\n","[DP-SGD] Epoch 14: train_loss=1.8766, val_loss=1.9388, val_acc=50.6480%, ε=6.6061\n","[DP-SGD] Epoch 15: train_loss=1.8202, val_loss=1.8733, val_acc=51.7620%, ε=6.8512\n","[DP-SGD] Epoch 16: train_loss=1.7477, val_loss=1.7975, val_acc=52.7060%, ε=7.0902\n","[DP-SGD] Epoch 17: train_loss=1.6615, val_loss=1.7203, val_acc=53.3560%, ε=7.3238\n","[DP-SGD] Epoch 18: train_loss=1.5578, val_loss=1.6385, val_acc=54.8280%, ε=7.5523\n","[DP-SGD] Epoch 19: train_loss=1.4915, val_loss=1.5570, val_acc=56.4000%, ε=7.7762\n","[DP-SGD] Epoch 20: train_loss=1.3798, val_loss=1.4751, val_acc=59.3400%, ε=7.9958\n","\n","[DP-SGD]\n","Best val acc: 59.3400%, Test acc: 60.4500%\n"," ε=7.9958\n","Total runtime: 232.0824 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(59.34, 60.45, np.float64(7.995831917202194))"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["### K-Medoids"],"metadata":{"id":"qjq6sXPoJ2qZ"}},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_CLUSTERS = 10\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"deuprcaSKil0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764493178746,"user_tz":480,"elapsed":26935,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"4fad519c-7999-4d51-c429-d68cc6a3be30"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:67: RuntimeWarning: divide by zero encountered in log\n","  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:70: RuntimeWarning: divide by zero encountered in log\n","  t > np.log(1 - q),\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3326, val_loss=2.3110, val_acc=8.6900%, ε=1.4451\n","[DP-SGD] Epoch 2: train_loss=2.3368, val_loss=2.3144, val_acc=8.8500%, ε=2.1200\n","[DP-SGD] Epoch 3: train_loss=2.3389, val_loss=2.3139, val_acc=9.3500%, ε=2.6597\n","[DP-SGD] Epoch 4: train_loss=2.3312, val_loss=2.3119, val_acc=10.2300%, ε=3.1286\n","[DP-SGD] Epoch 5: train_loss=2.3224, val_loss=2.3061, val_acc=10.7600%, ε=3.5518\n","[DP-SGD] Epoch 6: train_loss=2.3058, val_loss=2.3012, val_acc=10.1600%, ε=3.9423\n","[DP-SGD] Epoch 7: train_loss=2.2826, val_loss=2.2924, val_acc=10.2000%, ε=4.3079\n","[DP-SGD] Epoch 8: train_loss=2.2437, val_loss=2.2897, val_acc=9.8600%, ε=4.6538\n","[DP-SGD] Epoch 9: train_loss=2.2205, val_loss=2.2892, val_acc=10.2700%, ε=4.9834\n","[DP-SGD] Epoch 10: train_loss=2.1927, val_loss=2.2954, val_acc=10.2800%, ε=5.2994\n","\n","Early stopping at epoch 10 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 10.7600%, Test acc: 10.6000%\n"," ε=5.2994\n","Total runtime: 21.8153 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(10.76, 10.6, np.float64(5.2993806631722595))"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_CLUSTERS = 200\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"l6d9sVksKig5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763945515578,"user_tz":480,"elapsed":597462,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"ad159bd9-c82c-4044-b02a-f23aa5ed3367"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","/tmp/ipython-input-1715503071.py:10: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n","  loss.backward()\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3025, val_loss=2.3084, val_acc=9.9327%, ε=1.7625\n","[DP-SGD] Epoch 2: train_loss=2.3110, val_loss=2.3025, val_acc=9.9727%, ε=2.4240\n","[DP-SGD] Epoch 3: train_loss=2.3012, val_loss=2.2932, val_acc=10.1636%, ε=2.9456\n","[DP-SGD] Epoch 4: train_loss=2.2943, val_loss=2.2820, val_acc=10.7218%, ε=3.3954\n","[DP-SGD] Epoch 5: train_loss=2.2677, val_loss=2.2700, val_acc=12.8927%, ε=3.7996\n","[DP-SGD] Epoch 6: train_loss=2.2499, val_loss=2.2570, val_acc=17.2055%, ε=4.1714\n","[DP-SGD] Epoch 7: train_loss=2.2398, val_loss=2.2437, val_acc=20.5055%, ε=4.5186\n","[DP-SGD] Epoch 8: train_loss=2.2237, val_loss=2.2307, val_acc=24.4618%, ε=4.8463\n","[DP-SGD] Epoch 9: train_loss=2.2035, val_loss=2.2178, val_acc=28.1927%, ε=5.1581\n","[DP-SGD] Epoch 10: train_loss=2.1782, val_loss=2.2053, val_acc=30.8382%, ε=5.4565\n","[DP-SGD] Epoch 11: train_loss=2.1719, val_loss=2.1911, val_acc=32.8782%, ε=5.7435\n","[DP-SGD] Epoch 12: train_loss=2.1585, val_loss=2.1762, val_acc=34.9818%, ε=6.0207\n","[DP-SGD] Epoch 13: train_loss=2.1417, val_loss=2.1601, val_acc=38.0927%, ε=6.2892\n","[DP-SGD] Epoch 14: train_loss=2.1061, val_loss=2.1412, val_acc=40.2836%, ε=6.5499\n","[DP-SGD] Epoch 15: train_loss=2.0735, val_loss=2.1199, val_acc=41.7182%, ε=6.8038\n","[DP-SGD] Epoch 16: train_loss=2.0805, val_loss=2.0957, val_acc=43.0400%, ε=7.0515\n","[DP-SGD] Epoch 17: train_loss=2.0272, val_loss=2.0690, val_acc=43.5255%, ε=7.2935\n","[DP-SGD] Epoch 18: train_loss=1.9711, val_loss=2.0408, val_acc=44.4618%, ε=7.5303\n","[DP-SGD] Epoch 19: train_loss=1.9584, val_loss=2.0105, val_acc=45.4509%, ε=7.7625\n","[DP-SGD] Epoch 20: train_loss=1.9482, val_loss=1.9754, val_acc=46.8964%, ε=7.9902\n","\n","[DP-SGD]\n","Best val acc: 46.8964%, Test acc: 47.1100%\n"," ε=7.9902\n","Total runtime: 577.1307 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(46.89636363636364, 47.11, np.float64(7.990183309027119))"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_CLUSTERS = 300\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"sx2yN4lyKibZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763946111938,"user_tz":480,"elapsed":596359,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"fc7bee9f-c573-46ff-bdec-07383dc9759b"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","/tmp/ipython-input-1715503071.py:10: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n","  loss.backward()\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3021, val_loss=2.3055, val_acc=8.5036%, ε=1.9068\n","[DP-SGD] Epoch 2: train_loss=2.2906, val_loss=2.2966, val_acc=9.8618%, ε=2.5514\n","[DP-SGD] Epoch 3: train_loss=2.2763, val_loss=2.2831, val_acc=10.9582%, ε=3.0612\n","[DP-SGD] Epoch 4: train_loss=2.2631, val_loss=2.2673, val_acc=11.9800%, ε=3.5012\n","[DP-SGD] Epoch 5: train_loss=2.2399, val_loss=2.2497, val_acc=12.7873%, ε=3.8968\n","[DP-SGD] Epoch 6: train_loss=2.2179, val_loss=2.2305, val_acc=15.9782%, ε=4.2607\n","[DP-SGD] Epoch 7: train_loss=2.1792, val_loss=2.2092, val_acc=21.4400%, ε=4.6005\n","[DP-SGD] Epoch 8: train_loss=2.1620, val_loss=2.1855, val_acc=26.6418%, ε=4.9213\n","[DP-SGD] Epoch 9: train_loss=2.1347, val_loss=2.1602, val_acc=31.0836%, ε=5.2264\n","[DP-SGD] Epoch 10: train_loss=2.1184, val_loss=2.1311, val_acc=37.4236%, ε=5.5184\n","[DP-SGD] Epoch 11: train_loss=2.0647, val_loss=2.0985, val_acc=43.4109%, ε=5.7993\n","[DP-SGD] Epoch 12: train_loss=2.0232, val_loss=2.0597, val_acc=47.2400%, ε=6.0705\n","[DP-SGD] Epoch 13: train_loss=1.9878, val_loss=2.0163, val_acc=47.8473%, ε=6.3331\n","[DP-SGD] Epoch 14: train_loss=1.9282, val_loss=1.9679, val_acc=47.4073%, ε=6.5882\n","[DP-SGD] Epoch 15: train_loss=1.8790, val_loss=1.9173, val_acc=47.5218%, ε=6.8365\n","[DP-SGD] Epoch 16: train_loss=1.7836, val_loss=1.8607, val_acc=47.8891%, ε=7.0787\n","[DP-SGD] Epoch 17: train_loss=1.7455, val_loss=1.8000, val_acc=49.2636%, ε=7.3154\n","[DP-SGD] Epoch 18: train_loss=1.6103, val_loss=1.7367, val_acc=51.7509%, ε=7.5469\n","[DP-SGD] Epoch 19: train_loss=1.6147, val_loss=1.6710, val_acc=54.0091%, ε=7.7738\n","[DP-SGD] Epoch 20: train_loss=1.5128, val_loss=1.6046, val_acc=56.7127%, ε=7.9964\n","\n","[DP-SGD]\n","Best val acc: 56.7127%, Test acc: 57.5600%\n"," ε=7.9964\n","Total runtime: 579.1815 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(56.71272727272727, 57.56, np.float64(7.996436921674794))"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_CLUSTERS = 500\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6bpS9yPO6t3W","executionInfo":{"status":"ok","timestamp":1764492153991,"user_tz":480,"elapsed":107457,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"3ac2a948-1005-43f4-b0fc-1f1afcf70cdd"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3015, val_loss=2.2967, val_acc=10.0350%, ε=1.9909\n","[DP-SGD] Epoch 2: train_loss=2.2865, val_loss=2.2819, val_acc=13.3100%, ε=2.6240\n","[DP-SGD] Epoch 3: train_loss=2.2684, val_loss=2.2620, val_acc=17.7550%, ε=3.1260\n","[DP-SGD] Epoch 4: train_loss=2.2438, val_loss=2.2406, val_acc=22.5000%, ε=3.5598\n","[DP-SGD] Epoch 5: train_loss=2.2173, val_loss=2.2160, val_acc=26.9000%, ε=3.9499\n","[DP-SGD] Epoch 6: train_loss=2.1798, val_loss=2.1836, val_acc=30.9850%, ε=4.3089\n","[DP-SGD] Epoch 7: train_loss=2.1496, val_loss=2.1445, val_acc=37.2500%, ε=4.6443\n","[DP-SGD] Epoch 8: train_loss=2.1125, val_loss=2.0995, val_acc=44.7100%, ε=4.9609\n","[DP-SGD] Epoch 9: train_loss=2.0532, val_loss=2.0493, val_acc=52.3000%, ε=5.2620\n","[DP-SGD] Epoch 10: train_loss=1.9883, val_loss=1.9941, val_acc=55.8500%, ε=5.5503\n","[DP-SGD] Epoch 11: train_loss=1.9130, val_loss=1.9323, val_acc=58.1650%, ε=5.8275\n","[DP-SGD] Epoch 12: train_loss=1.8628, val_loss=1.8622, val_acc=59.9850%, ε=6.0951\n","[DP-SGD] Epoch 13: train_loss=1.7497, val_loss=1.7875, val_acc=61.7450%, ε=6.3544\n","[DP-SGD] Epoch 14: train_loss=1.6859, val_loss=1.7114, val_acc=62.9050%, ε=6.6061\n","[DP-SGD] Epoch 15: train_loss=1.5942, val_loss=1.6311, val_acc=64.0100%, ε=6.8512\n","[DP-SGD] Epoch 16: train_loss=1.5107, val_loss=1.5484, val_acc=66.3400%, ε=7.0902\n","[DP-SGD] Epoch 17: train_loss=1.3945, val_loss=1.4665, val_acc=68.1750%, ε=7.3238\n","[DP-SGD] Epoch 18: train_loss=1.3048, val_loss=1.3813, val_acc=69.4450%, ε=7.5523\n","[DP-SGD] Epoch 19: train_loss=1.2197, val_loss=1.2976, val_acc=70.6750%, ε=7.7762\n","[DP-SGD] Epoch 20: train_loss=1.1605, val_loss=1.2223, val_acc=71.2400%, ε=7.9958\n","\n","[DP-SGD]\n","Best val acc: 71.2400%, Test acc: 71.2800%\n"," ε=7.9958\n","Total runtime: 96.9852 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(71.24, 71.28, np.float64(7.995831917202194))"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_CLUSTERS = 500\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1yclV5F7puP","executionInfo":{"status":"ok","timestamp":1764492288643,"user_tz":480,"elapsed":64879,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"8a291319-1f92-401a-eb86-904e7d8e1171"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.2978, val_loss=2.2974, val_acc=10.1400%, ε=1.9909\n","[DP-SGD] Epoch 2: train_loss=2.2834, val_loss=2.2831, val_acc=13.0300%, ε=2.6240\n","[DP-SGD] Epoch 3: train_loss=2.2663, val_loss=2.2643, val_acc=14.0400%, ε=3.1260\n","[DP-SGD] Epoch 4: train_loss=2.2412, val_loss=2.2441, val_acc=13.8700%, ε=3.5598\n","[DP-SGD] Epoch 5: train_loss=2.2171, val_loss=2.2217, val_acc=14.3700%, ε=3.9499\n","[DP-SGD] Epoch 6: train_loss=2.1891, val_loss=2.1929, val_acc=17.3400%, ε=4.3089\n","[DP-SGD] Epoch 7: train_loss=2.1482, val_loss=2.1585, val_acc=22.8700%, ε=4.6443\n","[DP-SGD] Epoch 8: train_loss=2.1071, val_loss=2.1167, val_acc=31.9200%, ε=4.9609\n","[DP-SGD] Epoch 9: train_loss=2.0592, val_loss=2.0683, val_acc=40.1700%, ε=5.2620\n","[DP-SGD] Epoch 10: train_loss=1.9990, val_loss=2.0120, val_acc=46.8100%, ε=5.5503\n","[DP-SGD] Epoch 11: train_loss=1.9475, val_loss=1.9491, val_acc=52.5400%, ε=5.8275\n","[DP-SGD] Epoch 12: train_loss=1.8538, val_loss=1.8781, val_acc=57.2400%, ε=6.0951\n","[DP-SGD] Epoch 13: train_loss=1.7803, val_loss=1.7999, val_acc=62.7700%, ε=6.3544\n","[DP-SGD] Epoch 14: train_loss=1.7104, val_loss=1.7193, val_acc=66.1900%, ε=6.6061\n","[DP-SGD] Epoch 15: train_loss=1.6215, val_loss=1.6353, val_acc=67.7300%, ε=6.8512\n","[DP-SGD] Epoch 16: train_loss=1.5362, val_loss=1.5517, val_acc=69.0500%, ε=7.0902\n","[DP-SGD] Epoch 17: train_loss=1.4454, val_loss=1.4662, val_acc=70.2100%, ε=7.3238\n","[DP-SGD] Epoch 18: train_loss=1.3748, val_loss=1.3774, val_acc=71.3900%, ε=7.5523\n","[DP-SGD] Epoch 19: train_loss=1.2703, val_loss=1.2914, val_acc=72.4300%, ε=7.7762\n","[DP-SGD] Epoch 20: train_loss=1.1647, val_loss=1.2094, val_acc=72.9000%, ε=7.9958\n","\n","[DP-SGD]\n","Best val acc: 72.9000%, Test acc: 73.4900%\n"," ε=7.9958\n","Total runtime: 54.1429 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(72.9, 73.49, np.float64(7.995831917202194))"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_CLUSTERS = 5000\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2wtDnBML9dBh","executionInfo":{"status":"ok","timestamp":1764492893231,"user_tz":480,"elapsed":87217,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"1775b76d-c810-4fcd-946c-5f82c8c6523d"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.2242, val_loss=2.0802, val_acc=50.6100%, ε=2.7177\n","[DP-SGD] Epoch 2: train_loss=1.7895, val_loss=1.4029, val_acc=65.2500%, ε=3.2566\n","[DP-SGD] Epoch 3: train_loss=1.0957, val_loss=0.8283, val_acc=73.7300%, ε=3.6873\n","[DP-SGD] Epoch 4: train_loss=0.7106, val_loss=0.6205, val_acc=78.9500%, ε=4.0630\n","[DP-SGD] Epoch 5: train_loss=0.5977, val_loss=0.5512, val_acc=82.0800%, ε=4.4034\n","[DP-SGD] Epoch 6: train_loss=0.5416, val_loss=0.5399, val_acc=83.4600%, ε=4.7182\n","[DP-SGD] Epoch 7: train_loss=0.5147, val_loss=0.5207, val_acc=85.1500%, ε=5.0135\n","[DP-SGD] Epoch 8: train_loss=0.5508, val_loss=0.5131, val_acc=86.1800%, ε=5.2931\n","[DP-SGD] Epoch 9: train_loss=0.5531, val_loss=0.5057, val_acc=86.7000%, ε=5.5597\n","[DP-SGD] Epoch 10: train_loss=0.4933, val_loss=0.5174, val_acc=86.7900%, ε=5.8155\n","[DP-SGD] Epoch 11: train_loss=0.5131, val_loss=0.5285, val_acc=87.3900%, ε=6.0619\n","[DP-SGD] Epoch 12: train_loss=0.5380, val_loss=0.5270, val_acc=87.6800%, ε=6.3000\n","[DP-SGD] Epoch 13: train_loss=0.4960, val_loss=0.5380, val_acc=87.9300%, ε=6.5310\n","[DP-SGD] Epoch 14: train_loss=0.5176, val_loss=0.5314, val_acc=88.4100%, ε=6.7555\n","[DP-SGD] Epoch 15: train_loss=0.5502, val_loss=0.5394, val_acc=88.3900%, ε=6.9741\n","[DP-SGD] Epoch 16: train_loss=0.5387, val_loss=0.5423, val_acc=88.6300%, ε=7.1876\n","[DP-SGD] Epoch 17: train_loss=0.5622, val_loss=0.5494, val_acc=88.7200%, ε=7.3962\n","[DP-SGD] Epoch 18: train_loss=0.5335, val_loss=0.5452, val_acc=88.8900%, ε=7.6004\n","[DP-SGD] Epoch 19: train_loss=0.5333, val_loss=0.5463, val_acc=88.9500%, ε=7.8006\n","[DP-SGD] Epoch 20: train_loss=0.5222, val_loss=0.5519, val_acc=89.1500%, ε=7.9971\n","\n","[DP-SGD]\n","Best val acc: 89.1500%, Test acc: 89.6800%\n"," ε=7.9971\n","Total runtime: 78.9184 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(89.15, 89.68, np.float64(7.997052150973982))"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["## CIFAR-10"],"metadata":{"id":"_8NX3s-YJ4k2"}},{"cell_type":"markdown","source":["### Lazy Greedy"],"metadata":{"id":"6bNokVMmJ6Pu"}},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","\n","N_SAMPLES = 50\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"5F6_NFlKKVkz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763942611985,"user_tz":480,"elapsed":119060,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"6c94f9e7-af5e-42e4-d435-a22ffd5e466f"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 50.0/50.0 [00:00<00:00, 172it/s] \n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:67: RuntimeWarning: divide by zero encountered in log\n","  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:70: RuntimeWarning: divide by zero encountered in log\n","  t > np.log(1 - q),\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3183, val_loss=2.3029, val_acc=9.9822%, ε=1.4451\n","[DP-SGD] Epoch 2: train_loss=2.3173, val_loss=2.3029, val_acc=10.0489%, ε=2.1200\n","[DP-SGD] Epoch 3: train_loss=2.3154, val_loss=2.3030, val_acc=10.1889%, ε=2.6597\n","[DP-SGD] Epoch 4: train_loss=2.3128, val_loss=2.3030, val_acc=10.4733%, ε=3.1286\n","[DP-SGD] Epoch 5: train_loss=2.3098, val_loss=2.3030, val_acc=10.9022%, ε=3.5518\n","[DP-SGD] Epoch 6: train_loss=2.3064, val_loss=2.3030, val_acc=11.4244%, ε=3.9423\n","[DP-SGD] Epoch 7: train_loss=2.3021, val_loss=2.3030, val_acc=11.2400%, ε=4.3079\n","[DP-SGD] Epoch 8: train_loss=2.2972, val_loss=2.3029, val_acc=10.5356%, ε=4.6538\n","[DP-SGD] Epoch 9: train_loss=2.2920, val_loss=2.3029, val_acc=10.1444%, ε=4.9834\n","[DP-SGD] Epoch 10: train_loss=2.2865, val_loss=2.3030, val_acc=10.0289%, ε=5.2994\n","[DP-SGD] Epoch 11: train_loss=2.2808, val_loss=2.3032, val_acc=9.9756%, ε=5.6037\n","\n","Early stopping at epoch 11 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 11.4244%, Test acc: 11.0900%\n"," ε=5.6037\n","Total runtime: 111.5175 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(11.424444444444445, 11.09, np.float64(5.603719670759895))"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_SAMPLES = 100\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"_O-8eTaeKQbX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763943598899,"user_tz":480,"elapsed":26052,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"7183a525-9140-47a4-8891-bad24e754220"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 144it/s] \n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:67: RuntimeWarning: divide by zero encountered in log\n","  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:70: RuntimeWarning: divide by zero encountered in log\n","  t > np.log(1 - q),\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1715503071.py:10: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n","  loss.backward()\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3166, val_loss=2.3018, val_acc=11.0000%, ε=1.4451\n","[DP-SGD] Epoch 2: train_loss=2.3159, val_loss=2.3017, val_acc=10.7000%, ε=2.1200\n","[DP-SGD] Epoch 3: train_loss=2.3147, val_loss=2.3016, val_acc=11.0000%, ε=2.6597\n","[DP-SGD] Epoch 4: train_loss=2.3130, val_loss=2.3014, val_acc=11.5000%, ε=3.1286\n","[DP-SGD] Epoch 5: train_loss=2.3108, val_loss=2.3013, val_acc=11.0000%, ε=3.5518\n","[DP-SGD] Epoch 6: train_loss=2.3082, val_loss=2.3012, val_acc=11.5000%, ε=3.9423\n","[DP-SGD] Epoch 7: train_loss=2.3053, val_loss=2.3011, val_acc=10.7000%, ε=4.3079\n","[DP-SGD] Epoch 8: train_loss=2.3022, val_loss=2.3010, val_acc=9.5000%, ε=4.6538\n","[DP-SGD] Epoch 9: train_loss=2.2989, val_loss=2.3009, val_acc=9.8000%, ε=4.9834\n","\n","Early stopping at epoch 9 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 11.5000%, Test acc: 10.5500%\n"," ε=4.9834\n","Total runtime: 4.4745 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(11.5, 10.55, np.float64(4.9833854040797485))"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_SAMPLES = 200\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"EY9NLrozKQUl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763943643968,"user_tz":480,"elapsed":45057,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"f0e9e142-a757-4f70-a726-e948db6bd79a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [00:00<00:00, 361it/s]\n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","/tmp/ipython-input-1715503071.py:10: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n","  loss.backward()\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3009, val_loss=2.3056, val_acc=9.6000%, ε=1.7625\n","[DP-SGD] Epoch 2: train_loss=2.3006, val_loss=2.3055, val_acc=9.6000%, ε=2.4240\n","[DP-SGD] Epoch 3: train_loss=2.3015, val_loss=2.3055, val_acc=9.3000%, ε=2.9456\n","[DP-SGD] Epoch 4: train_loss=2.3032, val_loss=2.3053, val_acc=10.1000%, ε=3.3954\n","[DP-SGD] Epoch 5: train_loss=2.3002, val_loss=2.3053, val_acc=10.0000%, ε=3.7996\n","[DP-SGD] Epoch 6: train_loss=2.2977, val_loss=2.3053, val_acc=9.8000%, ε=4.1714\n","[DP-SGD] Epoch 7: train_loss=2.3008, val_loss=2.3053, val_acc=9.9000%, ε=4.5186\n","[DP-SGD] Epoch 8: train_loss=2.2968, val_loss=2.3052, val_acc=10.2000%, ε=4.8463\n","[DP-SGD] Epoch 9: train_loss=2.2968, val_loss=2.3051, val_acc=11.1000%, ε=5.1581\n","[DP-SGD] Epoch 10: train_loss=2.2958, val_loss=2.3051, val_acc=11.3000%, ε=5.4565\n","[DP-SGD] Epoch 11: train_loss=2.2925, val_loss=2.3049, val_acc=11.8000%, ε=5.7435\n","[DP-SGD] Epoch 12: train_loss=2.2977, val_loss=2.3049, val_acc=11.5000%, ε=6.0207\n","[DP-SGD] Epoch 13: train_loss=2.2856, val_loss=2.3049, val_acc=11.5000%, ε=6.2892\n","[DP-SGD] Epoch 14: train_loss=2.2854, val_loss=2.3050, val_acc=11.3000%, ε=6.5499\n","[DP-SGD] Epoch 15: train_loss=2.2867, val_loss=2.3052, val_acc=11.1000%, ε=6.8038\n","[DP-SGD] Epoch 16: train_loss=2.2946, val_loss=2.3053, val_acc=11.2000%, ε=7.0515\n","\n","Early stopping at epoch 16 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 11.8000%, Test acc: 10.9000%\n"," ε=7.0515\n","Total runtime: 20.0081 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(11.8, 10.9, np.float64(7.051483052991745))"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_SAMPLES = 300\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"SgKq0osBKQI_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763943689202,"user_tz":480,"elapsed":45232,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"71787f43-80c3-4a83-abda-9f22f271ea15"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 300/300 [00:00<00:00, 455it/s]\n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1715503071.py:10: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n","  loss.backward()\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3108, val_loss=2.3066, val_acc=8.7000%, ε=1.9068\n","[DP-SGD] Epoch 2: train_loss=2.3033, val_loss=2.3056, val_acc=9.8000%, ε=2.5514\n","[DP-SGD] Epoch 3: train_loss=2.3014, val_loss=2.3046, val_acc=10.3000%, ε=3.0612\n","[DP-SGD] Epoch 4: train_loss=2.2969, val_loss=2.3035, val_acc=10.6000%, ε=3.5012\n","[DP-SGD] Epoch 5: train_loss=2.3008, val_loss=2.3023, val_acc=11.0000%, ε=3.8968\n","[DP-SGD] Epoch 6: train_loss=2.2962, val_loss=2.3010, val_acc=11.5000%, ε=4.2607\n","[DP-SGD] Epoch 7: train_loss=2.2896, val_loss=2.2994, val_acc=12.4000%, ε=4.6005\n","[DP-SGD] Epoch 8: train_loss=2.2963, val_loss=2.2978, val_acc=13.4000%, ε=4.9213\n","[DP-SGD] Epoch 9: train_loss=2.2830, val_loss=2.2959, val_acc=13.6000%, ε=5.2264\n","[DP-SGD] Epoch 10: train_loss=2.2813, val_loss=2.2941, val_acc=14.8000%, ε=5.5184\n","[DP-SGD] Epoch 11: train_loss=2.2839, val_loss=2.2925, val_acc=14.1000%, ε=5.7993\n","[DP-SGD] Epoch 12: train_loss=2.2791, val_loss=2.2912, val_acc=15.3000%, ε=6.0705\n","[DP-SGD] Epoch 13: train_loss=2.2683, val_loss=2.2899, val_acc=15.5000%, ε=6.3331\n","[DP-SGD] Epoch 14: train_loss=2.2742, val_loss=2.2886, val_acc=16.2000%, ε=6.5882\n","[DP-SGD] Epoch 15: train_loss=2.2710, val_loss=2.2870, val_acc=16.3000%, ε=6.8365\n","[DP-SGD] Epoch 16: train_loss=2.2628, val_loss=2.2853, val_acc=16.7000%, ε=7.0787\n","[DP-SGD] Epoch 17: train_loss=2.2666, val_loss=2.2839, val_acc=16.2000%, ε=7.3154\n","[DP-SGD] Epoch 18: train_loss=2.2472, val_loss=2.2821, val_acc=16.8000%, ε=7.5469\n","[DP-SGD] Epoch 19: train_loss=2.2586, val_loss=2.2803, val_acc=16.6000%, ε=7.7738\n","[DP-SGD] Epoch 20: train_loss=2.2354, val_loss=2.2776, val_acc=16.3000%, ε=7.9964\n","\n","[DP-SGD]\n","Best val acc: 16.8000%, Test acc: 15.4300%\n"," ε=7.9964\n","Total runtime: 25.4866 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(16.8, 15.43, np.float64(7.996436921674794))"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_SAMPLES = 500\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"27AcKdgKPNfj","executionInfo":{"status":"ok","timestamp":1763944049686,"user_tz":480,"elapsed":38470,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"3b40471c-b4a6-4f47-f5ab-53c0ab470f04"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [00:02<00:00, 246it/s]\n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1715503071.py:10: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n","  loss.backward()\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.2994, val_loss=2.3072, val_acc=12.6000%, ε=1.9909\n","[DP-SGD] Epoch 2: train_loss=2.2991, val_loss=2.3067, val_acc=12.5000%, ε=2.6240\n","[DP-SGD] Epoch 3: train_loss=2.3008, val_loss=2.3061, val_acc=11.9500%, ε=3.1260\n","[DP-SGD] Epoch 4: train_loss=2.2996, val_loss=2.3053, val_acc=11.4500%, ε=3.5598\n","[DP-SGD] Epoch 5: train_loss=2.2908, val_loss=2.3045, val_acc=11.8000%, ε=3.9499\n","[DP-SGD] Epoch 6: train_loss=2.2914, val_loss=2.3039, val_acc=12.0000%, ε=4.3089\n","\n","Early stopping at epoch 6 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 12.6000%, Test acc: 12.2500%\n"," ε=4.3089\n","Total runtime: 10.1027 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(12.6, 12.25, np.float64(4.308944961647152))"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["### K-Medoids"],"metadata":{"id":"uic8mtPuJ8Vf"}},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_CLUSTERS = 100\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"L6ffP4TaKqc_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_CLUSTERS = 200\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"XN9Je2uKKqZj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_CLUSTERS = 300\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"uIoFuLQ4KqUV"},"execution_count":null,"outputs":[]}]}