{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NyIPeIWlJPow",
        "moU18r8FJb5R"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calebmk21/CS-260D/blob/main/DataSelection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Set-up:"
      ],
      "metadata": {
        "id": "vbc3jTbvIR3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opacus\n",
        "!pip install apricot-select numpy scikit-learn\n",
        "!pip install kmedoids"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wGgRT0_uINbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the CNN + DP-SGD\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import numpy as np\n",
        "from opacus import PrivacyEngine\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#For Lazy Greedy\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import heapq\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from apricot import FacilityLocationSelection\n",
        "import time\n",
        "\n",
        "#For K-medoids\n",
        "import kmedoids\n",
        "from sklearn.metrics.pairwise import euclidean_distances"
      ],
      "metadata": {
        "id": "_20F5IwnIxrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "NyIPeIWlJPow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        #=======================================#\n",
        "        #            MNIST Settings\n",
        "        #=======================================#\n",
        "\n",
        "        # self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
        "        # self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
        "        # self.fc1 = nn.Linear(32*12*12, 64)\n",
        "        # self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "        #=======================================#\n",
        "        #            CIFAR-10 Settings\n",
        "        #=======================================#\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=576, out_features=64)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = F.relu(self.conv1(x))\n",
        "        # x = F.relu(self.conv2(x))\n",
        "        # x = F.max_pool2d(x, 2)\n",
        "        # x = torch.flatten(x, 1)\n",
        "        # x = F.relu(self.fc1(x))\n",
        "        # return self.fc2(x)\n",
        "\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "eIp2H2PeJTFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST Model\n",
        "class CNN_MNIST(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        #=======================================#\n",
        "        #            MNIST Settings\n",
        "        #=======================================#\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
        "        self.fc1 = nn.Linear(32*12*12, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "\n",
        "# CIFAR-10 Model\n",
        "\n",
        "class CNN_CIFAR10(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        #=======================================#\n",
        "        #            CIFAR-10 Settings\n",
        "        #=======================================#\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=576, out_features=64)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "G4kxKosOJZvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer):\n",
        "    # train loop\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = F.cross_entropy(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ],
      "metadata": {
        "id": "7JgsYVwaJcn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    # evaluation\n",
        "    model.eval()\n",
        "    loss, correct = 0, 0\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        out = model(data)\n",
        "        loss += F.cross_entropy(out, target, reduction=\"sum\").item()\n",
        "        pred = out.argmax(1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "    loss /= len(loader.dataset)\n",
        "    acc = 100. * correct / len(loader.dataset)\n",
        "    return loss, acc"
      ],
      "metadata": {
        "id": "AhnJwpmuJfPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(dataset='MNIST', use_dp=False, fixed_privacy_budget=False):\n",
        "\n",
        "    # setup model and optimizer\n",
        "    if dataset == 'MNIST':\n",
        "      model = CNN_MNIST().to(device)\n",
        "    elif dataset == \"CIFAR\":\n",
        "      model = CNN_CIFAR10().to(device)\n",
        "\n",
        "    # Defaults to base CNN class if no dataset is specified\n",
        "    else:\n",
        "      model = CNN().to(device)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
        "\n",
        "    privacy_engine = None\n",
        "    if use_dp:\n",
        "\n",
        "        if fixed_privacy_budget:\n",
        "\n",
        "            privacy_engine = PrivacyEngine()\n",
        "\n",
        "            # Calculates sigma based on target epsilon and delta\n",
        "            model, optimizer, train_loader_dp = privacy_engine.make_private_with_epsilon(\n",
        "              module=model,\n",
        "              optimizer=optimizer,\n",
        "              data_loader=train_loader,\n",
        "              max_grad_norm=MAX_GRAD_NORM,\n",
        "              target_delta=TARGET_DELTA,\n",
        "              target_epsilon=TARGET_EPSILON,\n",
        "              epochs=NUM_EPOCHS\n",
        "        )\n",
        "        else:\n",
        "\n",
        "            privacy_engine = PrivacyEngine()\n",
        "\n",
        "            # Otherwise takes sigma as a hyperparameter\n",
        "            model, optimizer, train_loader_dp = privacy_engine.make_private(\n",
        "              module=model,\n",
        "              optimizer=optimizer,\n",
        "              data_loader=train_loader,\n",
        "              max_grad_norm=MAX_GRAD_NORM,\n",
        "              noise_multiplier=NOISE_MULTIPLIER,\n",
        "        )\n",
        "    else:\n",
        "        train_loader_dp = train_loader\n",
        "\n",
        "\n",
        "    # train\n",
        "    best_val_acc = 0\n",
        "    epochs_no_improve = 0\n",
        "    best_model_path = f\"best_model{'_dp' if use_dp else ''}.pt\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        train_loss = train_one_epoch(model, train_loader_dp, optimizer)\n",
        "        val_loss, val_acc = evaluate(model, val_loader)\n",
        "\n",
        "        eps = privacy_engine.get_epsilon(DELTA)\n",
        "\n",
        "        print(f\"[{'DP-SGD' if use_dp else 'Standard SGD'}] Epoch {epoch}: \"\n",
        "            + f\"train_loss={train_loss:.4f}, \"\n",
        "            + f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}%\"\n",
        "            + f\", ε={eps:.4f}\" if use_dp else \"\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= PATIENCE:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (no improvement for {PATIENCE} epochs).\")\n",
        "            break\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # test on best model\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    test_loss, test_acc = evaluate(model, test_loader)\n",
        "    final_eps = privacy_engine.get_epsilon(DELTA) if use_dp else None\n",
        "\n",
        "    print(f\"\\n[{ 'DP-SGD' if use_dp else 'Standard-SGD'}]\")\n",
        "    print(f\"Best val acc: {best_val_acc:.4f}%, Test acc: {test_acc:.4f}%\\n\" + (f\" ε={final_eps:.4f}\\n\" if use_dp else \"\") + (f\"Total runtime: {total_time:.4f} seconds\"))\n",
        "    return best_val_acc, test_acc, final_eps"
      ],
      "metadata": {
        "id": "r1paKWESJh7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Hyperparameters"
      ],
      "metadata": {
        "id": "zAmNVVXiJk3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LR = 0.01\n",
        "EPOCHS = 20\n",
        "SEED = 42\n",
        "MAX_GRAD_NORM = 1.0\n",
        "DELTA = 1e-5\n",
        "PATIENCE = 5\n",
        "\n",
        "# For dynamically calculated epsilon\n",
        "NOISE_MULTIPLIER = 1.0\n",
        "\n",
        "# For a fixed privacy budget (eps, del)\n",
        "TARGET_EPSILON = 8\n",
        "TARGET_DELTA = 1e-5\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "kBXzBBNvJlja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Selection"
      ],
      "metadata": {
        "id": "7xlsjHbHJAwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_dataset_greedy(name, n_samples):\n",
        "  if name == \"MNIST\":\n",
        "    transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),])\n",
        "\n",
        "    full_train = datasets.MNIST(\n",
        "        root=\"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_size = 5000\n",
        "    val_size = 1000\n",
        "    train_dataset, subset_rest = random_split(full_train, [train_size, len(full_train) - train_size])\n",
        "    val_dataset, _ = random_split(subset_rest, [val_size, len(subset_rest) - val_size])\n",
        "\n",
        "    test_dataset = datasets.MNIST(\n",
        "        root=\"./data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "  elif name == \"CIFAR\":\n",
        "    transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
        "\n",
        "\n",
        "    full_train = datasets.CIFAR10(\n",
        "        root=\"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_size = 40000\n",
        "    val_size = len(full_train) - train_size\n",
        "    train_dataset, subset_rest = random_split(full_train, [train_size, len(full_train) - train_size])\n",
        "    val_dataset, _ = random_split(subset_rest, [val_size, len(subset_rest) - val_size])\n",
        "\n",
        "\n",
        "    test_dataset = datasets.CIFAR10(\n",
        "        root=\"./data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "  train_data_array = train_dataset.dataset.data[train_dataset.indices]\n",
        "  # Convert to numpy array first if it's a tensor, then convert type\n",
        "  if isinstance(train_data_array, torch.Tensor):\n",
        "    train_data_array = train_data_array.cpu().numpy()\n",
        "\n",
        "  selector = FacilityLocationSelection(n_samples=n_samples, metric='euclidean', optimizer='lazy', verbose=True)\n",
        "\n",
        "  # Reshape the subset for selection\n",
        "  train_data_array = train_data_array.reshape(train_data_array.shape[0], -1)\n",
        "\n",
        "  # Use the subset for selection\n",
        "  selector.fit(train_data_array)\n",
        "\n",
        "  selected_indices = selector.ranking\n",
        "\n",
        "  selected_train_dataset = Subset(train_dataset, selected_indices)\n",
        "\n",
        "  train_loader = DataLoader(selected_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  val_loader   = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
        "  test_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "LE0RopAmJCQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_dataset_kmedoids(name, n_clusters):\n",
        "  if name == \"MNIST\":\n",
        "    transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),])\n",
        "\n",
        "    full_train = datasets.MNIST(\n",
        "        root=\"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_size = 50000\n",
        "    val_size = len(full_train) - train_size\n",
        "    train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n",
        "\n",
        "    test_dataset = datasets.MNIST(\n",
        "        root=\"./data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "  elif name == \"CIFAR\":\n",
        "    transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
        "\n",
        "\n",
        "    full_train = datasets.CIFAR10(\n",
        "        root=\"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_size = 40000\n",
        "    val_size = len(full_train) - train_size\n",
        "    train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n",
        "\n",
        "\n",
        "    test_dataset = datasets.CIFAR10(\n",
        "        root=\"./data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "\n",
        "  train_data_array = train_dataset.dataset.data[train_dataset.indices]\n",
        "  # Convert to numpy array first if it's a tensor, then convert type\n",
        "  if isinstance(train_data_array, torch.Tensor):\n",
        "    train_data_array = train_data_array.cpu().numpy()\n",
        "  train_data_array = train_data_array.astype('float32')\n",
        "  train_data_flat = train_data_array.reshape(train_data_array.shape[0], -1)\n",
        "\n",
        "  # Create a smaller subset for kmedoids to prevent crashes due to large memory usage\n",
        "  subset_size_for_kmedoids = min(5000, train_data_array.shape[0]) # Adjusted subset size\n",
        "  np.random.seed(SEED) # Ensure reproducibility\n",
        "  random_indices_kmedoids = np.random.choice(train_data_array.shape[0], subset_size_for_kmedoids, replace=False)\n",
        "  subset_for_kmedoids = train_data_flat[random_indices_kmedoids]\n",
        "\n",
        "  # Calculate distances only for the subset\n",
        "  diss = euclidean_distances(subset_for_kmedoids)\n",
        "\n",
        "  kmeds = kmedoids.KMedoids(n_clusters=n_clusters, method='fasterpam', random_state=0, metric='precomputed')\n",
        "  model = kmeds.fit(diss)\n",
        "\n",
        "  # Map the medoid indices from the subset back to the original dataset indices\n",
        "  selected_subset_indices = model.medoid_indices_\n",
        "  selected_original_indices = random_indices_kmedoids[selected_subset_indices]\n",
        "  select = [train_dataset.indices[i] for i in selected_original_indices]\n",
        "\n",
        "  selected_train_dataset = Subset(full_train, select)\n",
        "\n",
        "  train_loader = DataLoader(selected_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  val_loader   = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
        "  test_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "miRy9BYMJLbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test:"
      ],
      "metadata": {
        "id": "T2hlRjQ7Jt8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ],
      "metadata": {
        "id": "nrUn6qDmJx1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lazy Greedy"
      ],
      "metadata": {
        "id": "pTdS9vRvJ0R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"MNIST\"\n",
        "N_SAMPLES = 100\n",
        "train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for Lazy Greedy (100 samples)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "dLR_m83bKAZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"MNIST\"\n",
        "N_SAMPLES = 200\n",
        "train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for  Lazy Greedy (200 samples)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "LDwnuFQHKI_4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"MNIST\"\n",
        "N_SAMPLES = 300\n",
        "train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for Lazy Greedy (300 samples)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "ugaxcy1qKM_R",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Medoids"
      ],
      "metadata": {
        "id": "qjq6sXPoJ2qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"MNIST\"\n",
        "N_CLUSTERS = 100\n",
        "train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for K-Medoids (100 cluster)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "deuprcaSKil0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"MNIST\"\n",
        "N_CLUSTERS = 200\n",
        "train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for K-Medoids (200 cluster)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "l6d9sVksKig5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"MNIST\"\n",
        "N_CLUSTERS = 300\n",
        "train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for K-Medoids (300 cluster)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "sx2yN4lyKibZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR-10"
      ],
      "metadata": {
        "id": "_8NX3s-YJ4k2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lazy Greedy"
      ],
      "metadata": {
        "id": "6bNokVMmJ6Pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"CIFAR\"\n",
        "\n",
        "N_SAMPLES = 50\n",
        "train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for Lazy Greedy (50 samples)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "5F6_NFlKKVkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"CIFAR\"\n",
        "N_SAMPLES = 100\n",
        "train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for Lazy Greedy (100 samples)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "_O-8eTaeKQbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"CIFAR\"\n",
        "N_SAMPLES = 200\n",
        "train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for Lazy Greedy (200 samples)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "EY9NLrozKQUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"CIFAR\"\n",
        "N_SAMPLES = 300\n",
        "train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for Lazy Greedy (300 samples)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "SgKq0osBKQI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"CIFAR\"\n",
        "N_SAMPLES = 500\n",
        "train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for Lazy Greedy (500 samples)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "jInQzVSALOIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Medoids"
      ],
      "metadata": {
        "id": "uic8mtPuJ8Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"CIFAR\"\n",
        "N_CLUSTERS = 100\n",
        "train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for K-Medoids (100 cluster)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "L6ffP4TaKqc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"CIFAR\"\n",
        "N_CLUSTERS = 200\n",
        "train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for K-Medoids (200 cluster)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "XN9Je2uKKqZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"CIFAR\"\n",
        "N_CLUSTERS = 300\n",
        "train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for K-Medoids (300 cluster)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "uIoFuLQ4KqUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"CIFAR\"\n",
        "N_CLUSTERS = 500\n",
        "train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n",
        "\n",
        "print(f\"\\nRunning DP-SGD baseline for K-Medoids (500 cluster)\")\n",
        "run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"
      ],
      "metadata": {
        "id": "k8NnEXjoHocT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}