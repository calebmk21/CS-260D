{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1XuGLy_VOFfCZUuYph5oUVG_e100HAHhb","timestamp":1762919989525}],"collapsed_sections":["NyIPeIWlJPow","moU18r8FJb5R"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Initial Set-up:\n","This section installs and imports any required libraries that will be used in the code below:"],"metadata":{"id":"vbc3jTbvIR3l"}},{"cell_type":"code","source":["!pip install opacus\n","!pip install apricot-select numpy scikit-learn\n","!pip install kmedoids"],"metadata":{"collapsed":true,"id":"wGgRT0_uINbB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765149927225,"user_tz":480,"elapsed":23842,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}},"outputId":"317e033c-0bd3-4dec-d79c-a93afe875e20"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opacus\n","  Downloading opacus-1.5.4-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.12/dist-packages (from opacus) (2.0.2)\n","Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from opacus) (2.9.0+cu126)\n","Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.12/dist-packages (from opacus) (1.16.3)\n","Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from opacus) (3.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.5.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->opacus) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.3)\n","Downloading opacus-1.5.4-py3-none-any.whl (254 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.4/254.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: opacus\n","Successfully installed opacus-1.5.4\n","Collecting apricot-select\n","  Downloading apricot-select-0.6.1.tar.gz (28 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from apricot-select) (1.16.3)\n","Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.12/dist-packages (from apricot-select) (0.60.0)\n","Requirement already satisfied: tqdm>=4.24.0 in /usr/local/lib/python3.12/dist-packages (from apricot-select) (4.67.1)\n","Collecting nose (from apricot-select)\n","  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.43.0->apricot-select) (0.43.0)\n","Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: apricot-select\n","  Building wheel for apricot-select (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for apricot-select: filename=apricot_select-0.6.1-py3-none-any.whl size=48767 sha256=9d7f62ba1c0441e1bcf1f718bc27dd65b20eb781304e6e2e5b3f578cf80b4cd6\n","  Stored in directory: /root/.cache/pip/wheels/19/ce/18/f10e7debb348bf14d4ab90c8b657a91a79f1106b699a1121c3\n","Successfully built apricot-select\n","Installing collected packages: nose, apricot-select\n","Successfully installed apricot-select-0.6.1 nose-1.3.7\n","Collecting kmedoids\n","  Downloading kmedoids-0.5.4-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n","Downloading kmedoids-0.5.4-cp312-cp312-manylinux_2_28_x86_64.whl (476 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.3/476.3 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: kmedoids\n","Successfully installed kmedoids-0.5.4\n"]}]},{"cell_type":"code","source":["# For the CNN + DP-SGD\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, random_split, Subset\n","import numpy as np\n","from opacus import PrivacyEngine\n","import time\n","import matplotlib.pyplot as plt\n","\n","#For Lazy Greedy\n","from sklearn.metrics import pairwise_distances\n","import heapq\n","from sklearn.metrics import pairwise_distances\n","from apricot import FacilityLocationSelection\n","import time\n","\n","#For K-medoids\n","import kmedoids\n","from sklearn.metrics.pairwise import euclidean_distances"],"metadata":{"id":"_20F5IwnIxrp","executionInfo":{"status":"ok","timestamp":1765149956763,"user_tz":480,"elapsed":29536,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Select Dataset\n","All dataset arguments default to MNIST. This goes for the `run_experiment` function later. It is important to run `select_dataset` at least once as this loads the data. If you opt to run all cells, it will load MNIST by default."],"metadata":{"id":"7xlsjHbHJAwp"}},{"cell_type":"code","source":["#def select_dataset_greedy(name='MNIST', n_samples=100):\n","def select_dataset_greedy(name, n_samples):\n","  if name == \"MNIST\":\n","    transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,)),])\n","\n","    full_train = datasets.MNIST(\n","        root=\"./data\",\n","        train=True,\n","        download=True,\n","        transform=transform\n","    )\n","\n","    train_size = 5000\n","    val_size = 1000\n","    #val_size = len(full_train) - train_size\n","    # train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n","    train_dataset, subset_rest = random_split(full_train, [train_size, len(full_train) - train_size])\n","    val_dataset, _ = random_split(subset_rest, [val_size, len(subset_rest) - val_size])\n","\n","    test_dataset = datasets.MNIST(\n","        root=\"./data\",\n","        train=False,\n","        download=True,\n","        transform=transform\n","    )\n","\n","  elif name == \"CIFAR\":\n","    transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n","\n","\n","    full_train = datasets.CIFAR10(\n","        root=\"./data\",\n","        train=True,\n","        download=True,\n","        transform=transform\n","    )\n","\n","    train_size = 40000\n","    val_size = len(full_train) - train_size\n","    #train_size = 5000\n","    #val_size = 1000\n","    # train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n","    train_dataset, subset_rest = random_split(full_train, [train_size, len(full_train) - train_size])\n","    val_dataset, _ = random_split(subset_rest, [val_size, len(subset_rest) - val_size])\n","\n","\n","    test_dataset = datasets.CIFAR10(\n","        root=\"./data\",\n","        train=False,\n","        download=True,\n","        transform=transform\n","    )\n","\n","  train_data_array = train_dataset.dataset.data[train_dataset.indices]\n","\n","  #----------------------MADI ADDED BECAUSE RAM SUCKS---------------------------\n","\n","  #Create a smaller subset for FacilityLocationSelection to prevent crashes due to large memory usage\n","  #Adjusting the subset size for testing, e.g., 1000 samples\n","  subset_size_for_selection = min(1000, train_data_array.shape[0])\n","  # Randomly sample indices for the subset\n","  np.random.seed(SEED) # Ensure reproducibility\n","  random_indices = np.random.choice(train_data_array.shape[0], subset_size_for_selection, replace=False)\n","  subset_for_selection = train_data_array[random_indices]\n","\n","  #-----------------------------------------------------------------------------\n","\n","  selector = FacilityLocationSelection(n_samples=n_samples, metric='euclidean', optimizer='lazy', verbose=True)\n","\n","  # Reshape the subset for selection\n","  subset_for_selection = subset_for_selection.reshape(subset_for_selection.shape[0], -1)\n","  #train_data_array = train_data_array.reshape(train_data_array.shape[0], -1)\n","\n","  # Use the subset for selection\n","  selector.fit(subset_for_selection)\n","  #selector.fit(train_data_array)\n","\n","  selected_indices = selector.ranking\n","\n","  selected_train_dataset = Subset(train_dataset, selected_indices)\n","\n","  train_loader = DataLoader(selected_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","  val_loader   = DataLoader(val_dataset, batch_size=256, shuffle=False)\n","  test_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False)\n","\n","  return train_loader, val_loader, test_loader"],"metadata":{"id":"LE0RopAmJCQP","executionInfo":{"status":"ok","timestamp":1765150192031,"user_tz":480,"elapsed":44,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#def select_dataset_kmedoids(name='MNIST', n_clusters=300):\n","def select_dataset_kmedoids(name, n_clusters):\n","  if name == \"MNIST\":\n","    transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,)),])\n","\n","    full_train = datasets.MNIST(\n","        root=\"./data\",\n","        train=True,\n","        download=True,\n","        transform=transform\n","    )\n","\n","    train_size = 50000\n","    val_size = len(full_train) - train_size\n","    train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n","\n","    test_dataset = datasets.MNIST(\n","        root=\"./data\",\n","        train=False,\n","        download=True,\n","        transform=transform\n","    )\n","\n","  elif name == \"CIFAR\":\n","    transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n","\n","\n","    full_train = datasets.CIFAR10(\n","        root=\"./data\",\n","        train=True,\n","        download=True,\n","        transform=transform\n","    )\n","\n","    train_size = 40000\n","    val_size = len(full_train) - train_size\n","    train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n","\n","\n","    test_dataset = datasets.CIFAR10(\n","        root=\"./data\",\n","        train=False,\n","        download=True,\n","        transform=transform\n","    )\n","\n","  train_data_array = train_dataset.dataset.data[train_dataset.indices]\n","  # Convert to numpy array first if it's a tensor, then convert type\n","  if isinstance(train_data_array, torch.Tensor):\n","    train_data_array = train_data_array.cpu().numpy()\n","  train_data_array = train_data_array.astype('float32')\n","  train_data_flat = train_data_array.reshape(train_data_array.shape[0], -1)\n","\n","  # Create a smaller subset for kmedoids to prevent crashes due to large memory usage\n","  subset_size_for_kmedoids = min(5000, train_data_array.shape[0]) # Adjusted subset size\n","  np.random.seed(SEED) # Ensure reproducibility\n","  random_indices_kmedoids = np.random.choice(train_data_array.shape[0], subset_size_for_kmedoids, replace=False)\n","  subset_for_kmedoids = train_data_flat[random_indices_kmedoids]\n","\n","  # Calculate distances only for the subset\n","  diss = euclidean_distances(subset_for_kmedoids)\n","\n","  kmeds = kmedoids.KMedoids(n_clusters=n_clusters, method='fasterpam', random_state=0, metric='precomputed')\n","  model = kmeds.fit(diss)\n","\n","  # Map the medoid indices from the subset back to the original dataset indices\n","  selected_subset_indices = model.medoid_indices_\n","  selected_original_indices = random_indices_kmedoids[selected_subset_indices]\n","  select = [train_dataset.indices[i] for i in selected_original_indices]\n","\n","  selected_train_dataset = Subset(full_train, select)\n","\n","  train_loader = DataLoader(selected_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","  val_loader   = DataLoader(val_dataset, batch_size=256, shuffle=False)\n","  test_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False)\n","\n","  return train_loader, val_loader, test_loader"],"metadata":{"id":"miRy9BYMJLbq","executionInfo":{"status":"ok","timestamp":1765149956777,"user_tz":480,"elapsed":3,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Baseline CNN Model\n","\n","This model is chosen in `run_experiment` if an invalid dataset is selected. I should probably replace this with a `ThrowException` instead though."],"metadata":{"id":"NyIPeIWlJPow"}},{"cell_type":"code","source":["# baseline model\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        #=======================================#\n","        #            MNIST Settings\n","        #=======================================#\n","\n","        # self.conv1 = nn.Conv2d(1, 16, 3, 1)\n","        # self.conv2 = nn.Conv2d(16, 32, 3, 1)\n","        # self.fc1 = nn.Linear(32*12*12, 64)\n","        # self.fc2 = nn.Linear(64, 10)\n","\n","        #=======================================#\n","        #            CIFAR-10 Settings\n","        #=======================================#\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)\n","        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.fc1 = nn.Linear(in_features=576, out_features=64)\n","        self.fc2 = nn.Linear(in_features=64, out_features=10)\n","\n","    def forward(self, x):\n","        # x = F.relu(self.conv1(x))\n","        # x = F.relu(self.conv2(x))\n","        # x = F.max_pool2d(x, 2)\n","        # x = torch.flatten(x, 1)\n","        # x = F.relu(self.fc1(x))\n","        # return self.fc2(x)\n","\n","        x = self.pool(torch.relu(self.conv1(x)))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1)\n","\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n"],"metadata":{"id":"eIp2H2PeJTFN","executionInfo":{"status":"ok","timestamp":1765149956786,"user_tz":480,"elapsed":10,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Model Architectures\n","MNIST and CIFAR-10 require different NN structures, so they have their own separate classes that get initialized in `run_experiment`."],"metadata":{"id":"-3picnfHJVeL"}},{"cell_type":"code","source":["# MNIST Model\n","class CNN_MNIST(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        #=======================================#\n","        #            MNIST Settings\n","        #=======================================#\n","\n","        self.conv1 = nn.Conv2d(1, 16, 3, 1)\n","        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n","        self.fc1 = nn.Linear(32*12*12, 64)\n","        self.fc2 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2)\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        return self.fc2(x)\n","\n","\n","# CIFAR-10 Model\n","\n","class CNN_CIFAR10(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        #=======================================#\n","        #            CIFAR-10 Settings\n","        #=======================================#\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)\n","        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.fc1 = nn.Linear(in_features=576, out_features=64)\n","        self.fc2 = nn.Linear(in_features=64, out_features=10)\n","\n","    def forward(self, x):\n","\n","        x = self.pool(torch.relu(self.conv1(x)))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1)\n","\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n"],"metadata":{"id":"G4kxKosOJZvP","executionInfo":{"status":"ok","timestamp":1765149956788,"user_tz":480,"elapsed":9,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Model Training\n","Courtesy of Megha"],"metadata":{"id":"moU18r8FJb5R"}},{"cell_type":"code","source":["def train_one_epoch(model, loader, optimizer):\n","    # train loop\n","    model.train()\n","    total_loss = 0\n","    for batch_idx, (data, target) in enumerate(loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        out = model(data)\n","        loss = F.cross_entropy(out, target)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(loader)"],"metadata":{"id":"7JgsYVwaJcn0","executionInfo":{"status":"ok","timestamp":1765149956792,"user_tz":480,"elapsed":2,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def evaluate(model, loader):\n","    # evaluation\n","    model.eval()\n","    loss, correct = 0, 0\n","    for data, target in loader:\n","        data, target = data.to(device), target.to(device)\n","        out = model(data)\n","        loss += F.cross_entropy(out, target, reduction=\"sum\").item()\n","        pred = out.argmax(1)\n","        correct += pred.eq(target).sum().item()\n","    loss /= len(loader.dataset)\n","    acc = 100. * correct / len(loader.dataset)\n","    return loss, acc"],"metadata":{"id":"AhnJwpmuJfPg","executionInfo":{"status":"ok","timestamp":1765149956801,"user_tz":480,"elapsed":8,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","dataset: either MNIST or CIFAR\n","I set dataset_name as a hyperparameter we can use for convenience and easy swapping\n","Otherwise, it defaults to MNIST, and invalid datasets give the base CNN model\n","This makes the dataset selection a modular component, should we want to choose other datasets\n","\"\"\"\n","\n","def run_experiment(dataset='MNIST', use_dp=False, fixed_privacy_budget=False):\n","\n","    # setup model and optimizer\n","    if dataset == 'MNIST':\n","      model = CNN_MNIST().to(device)\n","    elif dataset == \"CIFAR\":\n","      model = CNN_CIFAR10().to(device)\n","\n","    # Defaults to base CNN class if no dataset is specified\n","    else:\n","      model = CNN().to(device)\n","\n","    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n","\n","    privacy_engine = None\n","    if use_dp:\n","\n","        if fixed_privacy_budget:\n","\n","            privacy_engine = PrivacyEngine()\n","\n","            # Calculates sigma based on target epsilon and delta\n","            model, optimizer, train_loader_dp = privacy_engine.make_private_with_epsilon(\n","              module=model,\n","              optimizer=optimizer,\n","              data_loader=train_loader,\n","              max_grad_norm=MAX_GRAD_NORM,\n","              target_delta=TARGET_DELTA,\n","              target_epsilon=TARGET_EPSILON,\n","              epochs=NUM_EPOCHS\n","        )\n","        else:\n","\n","            privacy_engine = PrivacyEngine()\n","\n","            # Otherwise takes sigma as a hyperparameter\n","            model, optimizer, train_loader_dp = privacy_engine.make_private(\n","              module=model,\n","              optimizer=optimizer,\n","              data_loader=train_loader,\n","              max_grad_norm=MAX_GRAD_NORM,\n","              noise_multiplier=NOISE_MULTIPLIER,\n","        )\n","    else:\n","        train_loader_dp = train_loader\n","\n","\n","    # train\n","    best_val_acc = 0\n","    epochs_no_improve = 0\n","    best_model_path = f\"best_model{'_dp' if use_dp else ''}.pt\"\n","\n","    start_time = time.time()\n","\n","    for epoch in range(1, EPOCHS + 1):\n","        train_loss = train_one_epoch(model, train_loader_dp, optimizer)\n","        val_loss, val_acc = evaluate(model, val_loader)\n","\n","        eps = privacy_engine.get_epsilon(DELTA)\n","\n","        print(f\"[{'DP-SGD' if use_dp else 'Standard SGD'}] Epoch {epoch}: \"\n","            + f\"train_loss={train_loss:.4f}, \"\n","            + f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}%\"\n","            + f\", ε={eps:.4f}\" if use_dp else \"\")\n","\n","        # if fixed_privacy_budget:\n","        #   eps = privacy_engine.get_epsilon(TARGET_DELTA)\n","        #   print(f\", ε={eps:.4f}\")\n","\n","        # elif use_dp:\n","        #   eps = privacy_engine.get_epsilon(DELTA)\n","        #   print()\n","\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            epochs_no_improve = 0\n","            torch.save(model.state_dict(), best_model_path)\n","        else:\n","            epochs_no_improve += 1\n","\n","        if epochs_no_improve >= PATIENCE:\n","            print(f\"\\nEarly stopping at epoch {epoch} (no improvement for {PATIENCE} epochs).\")\n","            break\n","\n","\n","    end_time = time.time()\n","    total_time = end_time - start_time\n","\n","    # test on best model\n","    model.load_state_dict(torch.load(best_model_path))\n","    test_loss, test_acc = evaluate(model, test_loader)\n","    final_eps = privacy_engine.get_epsilon(DELTA) if use_dp else None\n","\n","    print(f\"\\n[{ 'DP-SGD' if use_dp else 'Standard-SGD'}]\")\n","    print(f\"Best val acc: {best_val_acc:.4f}%, Test acc: {test_acc:.4f}%\\n\" + (f\" ε={final_eps:.4f}\\n\" if use_dp else \"\") + (f\"Total runtime: {total_time:.4f} seconds\"))\n","    return best_val_acc, test_acc, final_eps"],"metadata":{"id":"r1paKWESJh7n","executionInfo":{"status":"ok","timestamp":1765149956828,"user_tz":480,"elapsed":26,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Initial Benchmarks\n","## Model Hyperparameters"],"metadata":{"id":"zAmNVVXiJk3V"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","BATCH_SIZE = 128\n","LR = 0.01\n","EPOCHS = 20\n","SEED = 42\n","MAX_GRAD_NORM = 1.0\n","DELTA = 1e-5\n","PATIENCE = 5\n","\n","# For dynamically calculated epsilon\n","NOISE_MULTIPLIER = 1.0\n","\n","# For a fixed privacy budget (eps, del)\n","TARGET_EPSILON = 8\n","TARGET_DELTA = 1e-5\n","NUM_EPOCHS = 20\n","\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)"],"metadata":{"id":"kBXzBBNvJlja","executionInfo":{"status":"ok","timestamp":1765149956872,"user_tz":480,"elapsed":42,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# TESTS:"],"metadata":{"id":"T2hlRjQ7Jt8F"}},{"cell_type":"markdown","source":["## MNIST"],"metadata":{"id":"nrUn6qDmJx1H"}},{"cell_type":"markdown","source":["### Lazy Greedy"],"metadata":{"id":"pTdS9vRvJ0R2"}},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_SAMPLES = 100\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dLR_m83bKAZu","executionInfo":{"status":"ok","timestamp":1763940622727,"user_tz":480,"elapsed":13463,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"ed743708-048c-40d0-f25c-fea71c3ae08a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 191it/s] \n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:67: RuntimeWarning: divide by zero encountered in log\n","  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:70: RuntimeWarning: divide by zero encountered in log\n","  t > np.log(1 - q),\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3196, val_loss=2.3156, val_acc=7.3000%, ε=1.4451\n","[DP-SGD] Epoch 2: train_loss=2.3186, val_loss=2.3144, val_acc=7.5000%, ε=2.1200\n","[DP-SGD] Epoch 3: train_loss=2.3167, val_loss=2.3126, val_acc=8.8000%, ε=2.6597\n","[DP-SGD] Epoch 4: train_loss=2.3137, val_loss=2.3102, val_acc=10.2000%, ε=3.1286\n","[DP-SGD] Epoch 5: train_loss=2.3100, val_loss=2.3070, val_acc=11.4000%, ε=3.5518\n","[DP-SGD] Epoch 6: train_loss=2.3054, val_loss=2.3036, val_acc=13.0000%, ε=3.9423\n","[DP-SGD] Epoch 7: train_loss=2.3003, val_loss=2.3001, val_acc=14.3000%, ε=4.3079\n","[DP-SGD] Epoch 8: train_loss=2.2946, val_loss=2.2963, val_acc=14.6000%, ε=4.6538\n","[DP-SGD] Epoch 9: train_loss=2.2885, val_loss=2.2925, val_acc=16.3000%, ε=4.9834\n","[DP-SGD] Epoch 10: train_loss=2.2824, val_loss=2.2890, val_acc=17.1000%, ε=5.2994\n","[DP-SGD] Epoch 11: train_loss=2.2767, val_loss=2.2854, val_acc=17.7000%, ε=5.6037\n","[DP-SGD] Epoch 12: train_loss=2.2705, val_loss=2.2815, val_acc=18.1000%, ε=5.8979\n","[DP-SGD] Epoch 13: train_loss=2.2637, val_loss=2.2773, val_acc=18.4000%, ε=6.1833\n","[DP-SGD] Epoch 14: train_loss=2.2564, val_loss=2.2728, val_acc=19.0000%, ε=6.4607\n","[DP-SGD] Epoch 15: train_loss=2.2489, val_loss=2.2681, val_acc=19.4000%, ε=6.7310\n","[DP-SGD] Epoch 16: train_loss=2.2411, val_loss=2.2634, val_acc=19.8000%, ε=6.9949\n","[DP-SGD] Epoch 17: train_loss=2.2333, val_loss=2.2584, val_acc=20.7000%, ε=7.2530\n","[DP-SGD] Epoch 18: train_loss=2.2247, val_loss=2.2531, val_acc=21.8000%, ε=7.5058\n","[DP-SGD] Epoch 19: train_loss=2.2154, val_loss=2.2474, val_acc=23.9000%, ε=7.7537\n","[DP-SGD] Epoch 20: train_loss=2.2055, val_loss=2.2414, val_acc=25.7000%, ε=7.9971\n","\n","[DP-SGD]\n","Best val acc: 25.7000%, Test acc: 25.9400%\n"," ε=7.9971\n","Total runtime: 7.5060 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(25.7, 25.94, np.float64(7.997143272367276))"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_SAMPLES = 200\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LDwnuFQHKI_4","executionInfo":{"status":"ok","timestamp":1763940658924,"user_tz":480,"elapsed":33340,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"2354b3e4-2888-4fb9-f880-2650106e7c32","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [00:00<00:00, 325it/s]\n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3110, val_loss=2.3097, val_acc=9.6000%, ε=1.7625\n","[DP-SGD] Epoch 2: train_loss=2.2955, val_loss=2.3061, val_acc=9.5000%, ε=2.4240\n","[DP-SGD] Epoch 3: train_loss=2.2905, val_loss=2.3017, val_acc=9.5000%, ε=2.9456\n","[DP-SGD] Epoch 4: train_loss=2.2895, val_loss=2.2969, val_acc=9.9000%, ε=3.3954\n","[DP-SGD] Epoch 5: train_loss=2.2904, val_loss=2.2912, val_acc=10.2000%, ε=3.7996\n","[DP-SGD] Epoch 6: train_loss=2.2807, val_loss=2.2849, val_acc=10.4000%, ε=4.1714\n","[DP-SGD] Epoch 7: train_loss=2.2776, val_loss=2.2783, val_acc=11.5000%, ε=4.5186\n","[DP-SGD] Epoch 8: train_loss=2.2664, val_loss=2.2709, val_acc=14.0000%, ε=4.8463\n","[DP-SGD] Epoch 9: train_loss=2.2590, val_loss=2.2630, val_acc=15.5000%, ε=5.1581\n","[DP-SGD] Epoch 10: train_loss=2.2405, val_loss=2.2540, val_acc=16.8000%, ε=5.4565\n","[DP-SGD] Epoch 11: train_loss=2.2374, val_loss=2.2444, val_acc=18.4000%, ε=5.7435\n","[DP-SGD] Epoch 12: train_loss=2.2177, val_loss=2.2340, val_acc=19.1000%, ε=6.0207\n","[DP-SGD] Epoch 13: train_loss=2.2068, val_loss=2.2231, val_acc=21.5000%, ε=6.2892\n","[DP-SGD] Epoch 14: train_loss=2.1927, val_loss=2.2116, val_acc=23.2000%, ε=6.5499\n","[DP-SGD] Epoch 15: train_loss=2.1758, val_loss=2.1998, val_acc=26.3000%, ε=6.8038\n","[DP-SGD] Epoch 16: train_loss=2.1796, val_loss=2.1870, val_acc=27.6000%, ε=7.0515\n","[DP-SGD] Epoch 17: train_loss=2.1547, val_loss=2.1727, val_acc=28.7000%, ε=7.2935\n","[DP-SGD] Epoch 18: train_loss=2.1604, val_loss=2.1575, val_acc=29.8000%, ε=7.5303\n","[DP-SGD] Epoch 19: train_loss=2.1040, val_loss=2.1419, val_acc=29.9000%, ε=7.7625\n","[DP-SGD] Epoch 20: train_loss=2.0967, val_loss=2.1254, val_acc=32.0000%, ε=7.9902\n","\n","[DP-SGD]\n","Best val acc: 32.0000%, Test acc: 31.5900%\n"," ε=7.9902\n","Total runtime: 18.0514 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(32.0, 31.59, np.float64(7.990183308944778))"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_SAMPLES = 300\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugaxcy1qKM_R","executionInfo":{"status":"ok","timestamp":1763940717367,"user_tz":480,"elapsed":28454,"user":{"displayName":"KAYLEE LO","userId":"09750983471081514296"}},"outputId":"807866a2-2a59-451c-9ebc-96748a66e1d4","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 300/300 [00:00<00:00, 429it/s]\n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3145, val_loss=2.3070, val_acc=9.0000%, ε=1.9068\n","[DP-SGD] Epoch 2: train_loss=2.3010, val_loss=2.3017, val_acc=9.4000%, ε=2.5514\n","[DP-SGD] Epoch 3: train_loss=2.2920, val_loss=2.2954, val_acc=11.9000%, ε=3.0612\n","[DP-SGD] Epoch 4: train_loss=2.2875, val_loss=2.2889, val_acc=14.4000%, ε=3.5012\n","[DP-SGD] Epoch 5: train_loss=2.2682, val_loss=2.2822, val_acc=17.7000%, ε=3.8968\n","[DP-SGD] Epoch 6: train_loss=2.2603, val_loss=2.2745, val_acc=17.2000%, ε=4.2607\n","[DP-SGD] Epoch 7: train_loss=2.2438, val_loss=2.2660, val_acc=16.4000%, ε=4.6005\n","[DP-SGD] Epoch 8: train_loss=2.2267, val_loss=2.2560, val_acc=15.7000%, ε=4.9213\n","[DP-SGD] Epoch 9: train_loss=2.2106, val_loss=2.2428, val_acc=17.7000%, ε=5.2264\n","[DP-SGD] Epoch 10: train_loss=2.1731, val_loss=2.2284, val_acc=18.4000%, ε=5.5184\n","[DP-SGD] Epoch 11: train_loss=2.1610, val_loss=2.2128, val_acc=18.7000%, ε=5.7993\n","[DP-SGD] Epoch 12: train_loss=2.1325, val_loss=2.1963, val_acc=18.7000%, ε=6.0705\n","[DP-SGD] Epoch 13: train_loss=2.1290, val_loss=2.1765, val_acc=19.7000%, ε=6.3331\n","[DP-SGD] Epoch 14: train_loss=2.0959, val_loss=2.1524, val_acc=24.5000%, ε=6.5882\n","[DP-SGD] Epoch 15: train_loss=2.0447, val_loss=2.1265, val_acc=30.4000%, ε=6.8365\n","[DP-SGD] Epoch 16: train_loss=2.0365, val_loss=2.0962, val_acc=34.6000%, ε=7.0787\n","[DP-SGD] Epoch 17: train_loss=2.0039, val_loss=2.0597, val_acc=38.7000%, ε=7.3154\n","[DP-SGD] Epoch 18: train_loss=1.9359, val_loss=2.0199, val_acc=41.4000%, ε=7.5469\n","[DP-SGD] Epoch 19: train_loss=1.8837, val_loss=1.9789, val_acc=44.2000%, ε=7.7738\n","[DP-SGD] Epoch 20: train_loss=1.8223, val_loss=1.9380, val_acc=44.3000%, ε=7.9964\n","\n","[DP-SGD]\n","Best val acc: 44.3000%, Test acc: 45.0300%\n"," ε=7.9964\n","Total runtime: 16.6231 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(44.3, 45.03, np.float64(7.996436921675102))"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["### K-Medoids"],"metadata":{"id":"qjq6sXPoJ2qZ"}},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_CLUSTERS = 100\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"deuprcaSKil0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765150021483,"user_tz":480,"elapsed":64546,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}},"outputId":"531daee3-4a9c-4186-809a-49525c3c727e","collapsed":true},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:01<00:00, 5.53MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 131kB/s]\n","100%|██████████| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 9.73MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:67: RuntimeWarning: divide by zero encountered in log\n","  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:70: RuntimeWarning: divide by zero encountered in log\n","  t > np.log(1 - q),\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3022, val_loss=2.3034, val_acc=8.6700%, ε=1.4451\n","[DP-SGD] Epoch 2: train_loss=2.3014, val_loss=2.3018, val_acc=9.0300%, ε=2.1200\n","[DP-SGD] Epoch 3: train_loss=2.2994, val_loss=2.2995, val_acc=9.5700%, ε=2.6597\n","[DP-SGD] Epoch 4: train_loss=2.2961, val_loss=2.2965, val_acc=10.2900%, ε=3.1286\n","[DP-SGD] Epoch 5: train_loss=2.2920, val_loss=2.2928, val_acc=11.0200%, ε=3.5518\n","[DP-SGD] Epoch 6: train_loss=2.2870, val_loss=2.2884, val_acc=12.1600%, ε=3.9423\n","[DP-SGD] Epoch 7: train_loss=2.2811, val_loss=2.2837, val_acc=14.1900%, ε=4.3079\n","[DP-SGD] Epoch 8: train_loss=2.2746, val_loss=2.2783, val_acc=16.9900%, ε=4.6538\n","[DP-SGD] Epoch 9: train_loss=2.2672, val_loss=2.2723, val_acc=19.5100%, ε=4.9834\n","[DP-SGD] Epoch 10: train_loss=2.2590, val_loss=2.2661, val_acc=21.8100%, ε=5.2994\n","[DP-SGD] Epoch 11: train_loss=2.2506, val_loss=2.2596, val_acc=24.0900%, ε=5.6037\n","[DP-SGD] Epoch 12: train_loss=2.2419, val_loss=2.2532, val_acc=24.8600%, ε=5.8979\n","[DP-SGD] Epoch 13: train_loss=2.2332, val_loss=2.2468, val_acc=25.1500%, ε=6.1833\n","[DP-SGD] Epoch 14: train_loss=2.2245, val_loss=2.2402, val_acc=25.3800%, ε=6.4607\n","[DP-SGD] Epoch 15: train_loss=2.2154, val_loss=2.2333, val_acc=25.2600%, ε=6.7310\n","[DP-SGD] Epoch 16: train_loss=2.2061, val_loss=2.2265, val_acc=25.2700%, ε=6.9949\n","[DP-SGD] Epoch 17: train_loss=2.1968, val_loss=2.2196, val_acc=25.7100%, ε=7.2530\n","[DP-SGD] Epoch 18: train_loss=2.1876, val_loss=2.2126, val_acc=26.6800%, ε=7.5058\n","[DP-SGD] Epoch 19: train_loss=2.1780, val_loss=2.2050, val_acc=27.9500%, ε=7.7537\n","[DP-SGD] Epoch 20: train_loss=2.1679, val_loss=2.1968, val_acc=29.3000%, ε=7.9971\n","\n","[DP-SGD]\n","Best val acc: 29.3000%, Test acc: 29.3200%\n"," ε=7.9971\n","Total runtime: 46.9751 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(29.3, 29.32, np.float64(7.997143272367274))"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_CLUSTERS = 200\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"l6d9sVksKig5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765150088045,"user_tz":480,"elapsed":66561,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}},"outputId":"3c491a6f-e362-4512-ec82-161a620169c4","collapsed":true},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3085, val_loss=2.2995, val_acc=10.8900%, ε=1.7625\n","[DP-SGD] Epoch 2: train_loss=2.3036, val_loss=2.2975, val_acc=10.7200%, ε=2.4240\n","[DP-SGD] Epoch 3: train_loss=2.2924, val_loss=2.2953, val_acc=10.9200%, ε=2.9456\n","[DP-SGD] Epoch 4: train_loss=2.2881, val_loss=2.2925, val_acc=11.2900%, ε=3.3954\n","[DP-SGD] Epoch 5: train_loss=2.2889, val_loss=2.2896, val_acc=11.7100%, ε=3.7996\n","[DP-SGD] Epoch 6: train_loss=2.2885, val_loss=2.2865, val_acc=11.3700%, ε=4.1714\n","[DP-SGD] Epoch 7: train_loss=2.2775, val_loss=2.2835, val_acc=10.9300%, ε=4.5186\n","[DP-SGD] Epoch 8: train_loss=2.2666, val_loss=2.2804, val_acc=10.8700%, ε=4.8463\n","[DP-SGD] Epoch 9: train_loss=2.2773, val_loss=2.2768, val_acc=11.1700%, ε=5.1581\n","[DP-SGD] Epoch 10: train_loss=2.2641, val_loss=2.2725, val_acc=12.8300%, ε=5.4565\n","[DP-SGD] Epoch 11: train_loss=2.2537, val_loss=2.2679, val_acc=15.7000%, ε=5.7435\n","[DP-SGD] Epoch 12: train_loss=2.2482, val_loss=2.2626, val_acc=17.3200%, ε=6.0207\n","[DP-SGD] Epoch 13: train_loss=2.2414, val_loss=2.2569, val_acc=17.3000%, ε=6.2892\n","[DP-SGD] Epoch 14: train_loss=2.2381, val_loss=2.2512, val_acc=15.9000%, ε=6.5499\n","[DP-SGD] Epoch 15: train_loss=2.2241, val_loss=2.2449, val_acc=15.0400%, ε=6.8038\n","[DP-SGD] Epoch 16: train_loss=2.2104, val_loss=2.2381, val_acc=14.6300%, ε=7.0515\n","[DP-SGD] Epoch 17: train_loss=2.2230, val_loss=2.2301, val_acc=15.0000%, ε=7.2935\n","\n","Early stopping at epoch 17 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 17.3200%, Test acc: 17.9200%\n"," ε=7.2935\n","Total runtime: 50.1122 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(17.32, 17.92, np.float64(7.293493928359672))"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["DATASET_NAME = \"MNIST\"\n","N_CLUSTERS = 300\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"sx2yN4lyKibZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765150158288,"user_tz":480,"elapsed":70241,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}},"outputId":"f1f7b85a-7bb3-44c2-e373-9aeff77cac6f","collapsed":true},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.2987, val_loss=2.3066, val_acc=9.6000%, ε=1.9068\n","[DP-SGD] Epoch 2: train_loss=2.2924, val_loss=2.2981, val_acc=10.8900%, ε=2.5514\n","[DP-SGD] Epoch 3: train_loss=2.2893, val_loss=2.2874, val_acc=12.9600%, ε=3.0612\n","[DP-SGD] Epoch 4: train_loss=2.2746, val_loss=2.2752, val_acc=15.7600%, ε=3.5012\n","[DP-SGD] Epoch 5: train_loss=2.2601, val_loss=2.2614, val_acc=18.9700%, ε=3.8968\n","[DP-SGD] Epoch 6: train_loss=2.2454, val_loss=2.2457, val_acc=22.2600%, ε=4.2607\n","[DP-SGD] Epoch 7: train_loss=2.2162, val_loss=2.2265, val_acc=26.0800%, ε=4.6005\n","[DP-SGD] Epoch 8: train_loss=2.1883, val_loss=2.2052, val_acc=29.4300%, ε=4.9213\n","[DP-SGD] Epoch 9: train_loss=2.1875, val_loss=2.1811, val_acc=33.7600%, ε=5.2264\n","[DP-SGD] Epoch 10: train_loss=2.1396, val_loss=2.1519, val_acc=38.4600%, ε=5.5184\n","[DP-SGD] Epoch 11: train_loss=2.1240, val_loss=2.1206, val_acc=42.1300%, ε=5.7993\n","[DP-SGD] Epoch 12: train_loss=2.0665, val_loss=2.0879, val_acc=45.9100%, ε=6.0705\n","[DP-SGD] Epoch 13: train_loss=2.0274, val_loss=2.0498, val_acc=49.7300%, ε=6.3331\n","[DP-SGD] Epoch 14: train_loss=1.9807, val_loss=2.0057, val_acc=52.7300%, ε=6.5882\n","[DP-SGD] Epoch 15: train_loss=1.9332, val_loss=1.9567, val_acc=55.5400%, ε=6.8365\n","[DP-SGD] Epoch 16: train_loss=1.9004, val_loss=1.9029, val_acc=59.3000%, ε=7.0787\n","[DP-SGD] Epoch 17: train_loss=1.8072, val_loss=1.8455, val_acc=61.4100%, ε=7.3154\n","[DP-SGD] Epoch 18: train_loss=1.7347, val_loss=1.7845, val_acc=62.8800%, ε=7.5469\n","[DP-SGD] Epoch 19: train_loss=1.6958, val_loss=1.7221, val_acc=64.0200%, ε=7.7738\n","[DP-SGD] Epoch 20: train_loss=1.6268, val_loss=1.6578, val_acc=65.1600%, ε=7.9964\n","\n","[DP-SGD]\n","Best val acc: 65.1600%, Test acc: 65.8800%\n"," ε=7.9964\n","Total runtime: 57.1810 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(65.16, 65.88, np.float64(7.9964369216751034))"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["## CIFAR-10"],"metadata":{"id":"_8NX3s-YJ4k2"}},{"cell_type":"markdown","source":["### Lazy Greedy"],"metadata":{"id":"6bNokVMmJ6Pu"}},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","\n","N_SAMPLES = 50\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"5F6_NFlKKVkz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765150236869,"user_tz":480,"elapsed":38703,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}},"outputId":"6cb9965d-146a-4c0f-c0ef-a001d3a3811f"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:13<00:00, 12.8MB/s]\n","100%|██████████| 50.0/50.0 [00:00<00:00, 1.12kit/s]\n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:67: RuntimeWarning: divide by zero encountered in log\n","  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:70: RuntimeWarning: divide by zero encountered in log\n","  t > np.log(1 - q),\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3100, val_loss=2.3071, val_acc=10.5000%, ε=1.4451\n","[DP-SGD] Epoch 2: train_loss=2.3097, val_loss=2.3071, val_acc=10.4100%, ε=2.1200\n","[DP-SGD] Epoch 3: train_loss=2.3090, val_loss=2.3071, val_acc=10.4100%, ε=2.6597\n","[DP-SGD] Epoch 4: train_loss=2.3081, val_loss=2.3071, val_acc=10.3900%, ε=3.1286\n","[DP-SGD] Epoch 5: train_loss=2.3070, val_loss=2.3071, val_acc=10.1500%, ε=3.5518\n","[DP-SGD] Epoch 6: train_loss=2.3054, val_loss=2.3072, val_acc=9.9700%, ε=3.9423\n","\n","Early stopping at epoch 6 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 10.5000%, Test acc: 10.9000%\n"," ε=3.9423\n","Total runtime: 14.7384 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(10.5, 10.9, np.float64(3.942293752673243))"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_SAMPLES = 100\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"_O-8eTaeKQbX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765150263638,"user_tz":480,"elapsed":26767,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}},"outputId":"de187dd9-129c-4933-be04-3c69b783f6b3"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:00<00:00, 1.64kit/s]\n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:67: RuntimeWarning: divide by zero encountered in log\n","  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:70: RuntimeWarning: divide by zero encountered in log\n","  t > np.log(1 - q),\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3182, val_loss=2.3072, val_acc=10.0500%, ε=1.4451\n","[DP-SGD] Epoch 2: train_loss=2.3181, val_loss=2.3072, val_acc=10.0500%, ε=2.1200\n","[DP-SGD] Epoch 3: train_loss=2.3177, val_loss=2.3070, val_acc=10.0900%, ε=2.6597\n","[DP-SGD] Epoch 4: train_loss=2.3171, val_loss=2.3069, val_acc=10.0400%, ε=3.1286\n","[DP-SGD] Epoch 5: train_loss=2.3164, val_loss=2.3066, val_acc=9.9200%, ε=3.5518\n","[DP-SGD] Epoch 6: train_loss=2.3154, val_loss=2.3064, val_acc=9.9400%, ε=3.9423\n","[DP-SGD] Epoch 7: train_loss=2.3143, val_loss=2.3062, val_acc=9.8800%, ε=4.3079\n","[DP-SGD] Epoch 8: train_loss=2.3132, val_loss=2.3060, val_acc=9.9700%, ε=4.6538\n","\n","Early stopping at epoch 8 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 10.0900%, Test acc: 10.3100%\n"," ε=4.6538\n","Total runtime: 19.9943 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(10.09, 10.31, np.float64(4.65376554171542))"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_SAMPLES = 200\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"EY9NLrozKQUl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765150322173,"user_tz":480,"elapsed":58555,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}},"outputId":"7f2103ca-35eb-44d8-a7ca-7755e4c7085b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [00:00<00:00, 1.96kit/s]\n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3042, val_loss=2.3034, val_acc=10.0700%, ε=1.7625\n","[DP-SGD] Epoch 2: train_loss=2.3084, val_loss=2.3032, val_acc=10.1200%, ε=2.4240\n","[DP-SGD] Epoch 3: train_loss=2.3065, val_loss=2.3030, val_acc=10.1600%, ε=2.9456\n","[DP-SGD] Epoch 4: train_loss=2.3103, val_loss=2.3027, val_acc=10.2100%, ε=3.3954\n","[DP-SGD] Epoch 5: train_loss=2.3032, val_loss=2.3025, val_acc=10.1400%, ε=3.7996\n","[DP-SGD] Epoch 6: train_loss=2.3003, val_loss=2.3022, val_acc=10.1800%, ε=4.1714\n","[DP-SGD] Epoch 7: train_loss=2.3043, val_loss=2.3020, val_acc=10.2900%, ε=4.5186\n","[DP-SGD] Epoch 8: train_loss=2.3025, val_loss=2.3017, val_acc=10.5900%, ε=4.8463\n","[DP-SGD] Epoch 9: train_loss=2.3016, val_loss=2.3014, val_acc=10.0100%, ε=5.1581\n","[DP-SGD] Epoch 10: train_loss=2.3009, val_loss=2.3011, val_acc=9.9000%, ε=5.4565\n","[DP-SGD] Epoch 11: train_loss=2.2939, val_loss=2.3008, val_acc=10.0400%, ε=5.7435\n","[DP-SGD] Epoch 12: train_loss=2.2939, val_loss=2.3005, val_acc=10.0300%, ε=6.0207\n","[DP-SGD] Epoch 13: train_loss=2.2972, val_loss=2.3004, val_acc=10.2300%, ε=6.2892\n","\n","Early stopping at epoch 13 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 10.5900%, Test acc: 10.6400%\n"," ε=6.2892\n","Total runtime: 41.2196 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(10.59, 10.64, np.float64(6.2891640598459615))"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_SAMPLES = 300\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"SgKq0osBKQI_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765150354509,"user_tz":480,"elapsed":32325,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}},"outputId":"170211ca-3be0-441a-b8e7-525c7c591825"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 300/300 [00:00<00:00, 1.77kit/s]\n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3072, val_loss=2.3050, val_acc=9.1000%, ε=1.9068\n","[DP-SGD] Epoch 2: train_loss=2.3081, val_loss=2.3047, val_acc=9.0200%, ε=2.5514\n","[DP-SGD] Epoch 3: train_loss=2.3062, val_loss=2.3044, val_acc=8.6700%, ε=3.0612\n","[DP-SGD] Epoch 4: train_loss=2.3040, val_loss=2.3039, val_acc=8.8900%, ε=3.5012\n","[DP-SGD] Epoch 5: train_loss=2.3053, val_loss=2.3033, val_acc=9.0600%, ε=3.8968\n","[DP-SGD] Epoch 6: train_loss=2.3000, val_loss=2.3028, val_acc=9.0600%, ε=4.2607\n","\n","Early stopping at epoch 6 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 9.1000%, Test acc: 9.4000%\n"," ε=4.2607\n","Total runtime: 19.4887 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(9.1, 9.4, np.float64(4.260672744324618))"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_SAMPLES = 500\n","train_loader, val_loader, test_loader = select_dataset_greedy(DATASET_NAME, N_SAMPLES)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jInQzVSALOIk","executionInfo":{"status":"ok","timestamp":1765150676634,"user_tz":480,"elapsed":57777,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}},"outputId":"fd84fa56-8c93-46c3-d486-961fc5c1dbee"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [00:00<00:00, 1.33kit/s]\n","/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3039, val_loss=2.3064, val_acc=9.7400%, ε=1.9909\n","[DP-SGD] Epoch 2: train_loss=2.3033, val_loss=2.3060, val_acc=9.6900%, ε=2.6240\n","[DP-SGD] Epoch 3: train_loss=2.3047, val_loss=2.3056, val_acc=9.7000%, ε=3.1260\n","[DP-SGD] Epoch 4: train_loss=2.2962, val_loss=2.3050, val_acc=9.8700%, ε=3.5598\n","[DP-SGD] Epoch 5: train_loss=2.2977, val_loss=2.3043, val_acc=10.1900%, ε=3.9499\n","[DP-SGD] Epoch 6: train_loss=2.3010, val_loss=2.3035, val_acc=11.2300%, ε=4.3089\n","[DP-SGD] Epoch 7: train_loss=2.2985, val_loss=2.3025, val_acc=13.3500%, ε=4.6443\n","[DP-SGD] Epoch 8: train_loss=2.2974, val_loss=2.3016, val_acc=15.7900%, ε=4.9609\n","[DP-SGD] Epoch 9: train_loss=2.2937, val_loss=2.3007, val_acc=16.0200%, ε=5.2620\n","[DP-SGD] Epoch 10: train_loss=2.2925, val_loss=2.2997, val_acc=15.6800%, ε=5.5503\n","[DP-SGD] Epoch 11: train_loss=2.2897, val_loss=2.2987, val_acc=15.2100%, ε=5.8275\n","[DP-SGD] Epoch 12: train_loss=2.2878, val_loss=2.2977, val_acc=14.6500%, ε=6.0951\n","[DP-SGD] Epoch 13: train_loss=2.2832, val_loss=2.2968, val_acc=14.4400%, ε=6.3544\n","[DP-SGD] Epoch 14: train_loss=2.2886, val_loss=2.2956, val_acc=14.5000%, ε=6.6061\n","\n","Early stopping at epoch 14 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 16.0200%, Test acc: 15.5200%\n"," ε=6.6061\n","Total runtime: 44.2794 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(16.02, 15.52, np.float64(6.60613552545733))"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["### K-Medoids"],"metadata":{"id":"uic8mtPuJ8Vf"}},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_CLUSTERS = 100\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"L6ffP4TaKqc_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765150394253,"user_tz":480,"elapsed":39742,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}},"outputId":"73cd839a-0a66-4097-9ef6-b2f8a391b120"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:67: RuntimeWarning: divide by zero encountered in log\n","  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/prvs.py:70: RuntimeWarning: divide by zero encountered in log\n","  t > np.log(1 - q),\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3057, val_loss=2.3064, val_acc=10.3100%, ε=1.4451\n","[DP-SGD] Epoch 2: train_loss=2.3051, val_loss=2.3064, val_acc=10.3200%, ε=2.1200\n","[DP-SGD] Epoch 3: train_loss=2.3040, val_loss=2.3064, val_acc=10.5400%, ε=2.6597\n","[DP-SGD] Epoch 4: train_loss=2.3024, val_loss=2.3064, val_acc=10.6000%, ε=3.1286\n","[DP-SGD] Epoch 5: train_loss=2.3004, val_loss=2.3064, val_acc=10.7700%, ε=3.5518\n","[DP-SGD] Epoch 6: train_loss=2.2981, val_loss=2.3064, val_acc=10.9500%, ε=3.9423\n","[DP-SGD] Epoch 7: train_loss=2.2955, val_loss=2.3064, val_acc=10.9500%, ε=4.3079\n","[DP-SGD] Epoch 8: train_loss=2.2927, val_loss=2.3064, val_acc=10.9000%, ε=4.6538\n","[DP-SGD] Epoch 9: train_loss=2.2894, val_loss=2.3065, val_acc=10.7700%, ε=4.9834\n","[DP-SGD] Epoch 10: train_loss=2.2858, val_loss=2.3065, val_acc=10.6300%, ε=5.2994\n","[DP-SGD] Epoch 11: train_loss=2.2821, val_loss=2.3067, val_acc=10.6000%, ε=5.6037\n","\n","Early stopping at epoch 11 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 10.9500%, Test acc: 11.2100%\n"," ε=5.6037\n","Total runtime: 29.2025 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(10.95, 11.21, np.float64(5.603719670759895))"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_CLUSTERS = 200\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"XN9Je2uKKqZj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765150439155,"user_tz":480,"elapsed":44900,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}},"outputId":"e7d63a43-915e-42ec-e19b-1a7976b27984"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3177, val_loss=2.3053, val_acc=10.0400%, ε=1.7625\n","[DP-SGD] Epoch 2: train_loss=2.3148, val_loss=2.3053, val_acc=10.0500%, ε=2.4240\n","[DP-SGD] Epoch 3: train_loss=2.3132, val_loss=2.3052, val_acc=9.7200%, ε=2.9456\n","[DP-SGD] Epoch 4: train_loss=2.3092, val_loss=2.3052, val_acc=9.4900%, ε=3.3954\n","[DP-SGD] Epoch 5: train_loss=2.3116, val_loss=2.3051, val_acc=9.3500%, ε=3.7996\n","[DP-SGD] Epoch 6: train_loss=2.2991, val_loss=2.3051, val_acc=9.2400%, ε=4.1714\n","[DP-SGD] Epoch 7: train_loss=2.3049, val_loss=2.3052, val_acc=9.2200%, ε=4.5186\n","\n","Early stopping at epoch 7 (no improvement for 5 epochs).\n","\n","[DP-SGD]\n","Best val acc: 10.0500%, Test acc: 9.7000%\n"," ε=4.5186\n","Total runtime: 23.0196 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(10.05, 9.7, np.float64(4.518568053726991))"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_CLUSTERS = 300\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"id":"uIoFuLQ4KqUV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765150520740,"user_tz":480,"elapsed":81583,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}},"outputId":"179a5850-5d09-4089-99eb-2205cd9c9929"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3041, val_loss=2.3050, val_acc=9.8100%, ε=1.9068\n","[DP-SGD] Epoch 2: train_loss=2.3024, val_loss=2.3046, val_acc=9.8400%, ε=2.5514\n","[DP-SGD] Epoch 3: train_loss=2.3016, val_loss=2.3040, val_acc=10.0100%, ε=3.0612\n","[DP-SGD] Epoch 4: train_loss=2.3017, val_loss=2.3031, val_acc=10.1600%, ε=3.5012\n","[DP-SGD] Epoch 5: train_loss=2.2999, val_loss=2.3022, val_acc=10.3000%, ε=3.8968\n","[DP-SGD] Epoch 6: train_loss=2.2956, val_loss=2.3011, val_acc=10.5900%, ε=4.2607\n","[DP-SGD] Epoch 7: train_loss=2.2876, val_loss=2.3000, val_acc=10.8200%, ε=4.6005\n","[DP-SGD] Epoch 8: train_loss=2.2894, val_loss=2.2989, val_acc=10.9800%, ε=4.9213\n","[DP-SGD] Epoch 9: train_loss=2.2900, val_loss=2.2979, val_acc=11.3900%, ε=5.2264\n","[DP-SGD] Epoch 10: train_loss=2.2858, val_loss=2.2972, val_acc=11.5200%, ε=5.5184\n","[DP-SGD] Epoch 11: train_loss=2.2855, val_loss=2.2964, val_acc=11.5300%, ε=5.7993\n","[DP-SGD] Epoch 12: train_loss=2.2848, val_loss=2.2954, val_acc=11.6500%, ε=6.0705\n","[DP-SGD] Epoch 13: train_loss=2.2745, val_loss=2.2941, val_acc=11.9400%, ε=6.3331\n","[DP-SGD] Epoch 14: train_loss=2.2729, val_loss=2.2929, val_acc=12.4400%, ε=6.5882\n","[DP-SGD] Epoch 15: train_loss=2.2714, val_loss=2.2916, val_acc=13.7600%, ε=6.8365\n","[DP-SGD] Epoch 16: train_loss=2.2675, val_loss=2.2902, val_acc=15.1400%, ε=7.0787\n","[DP-SGD] Epoch 17: train_loss=2.2666, val_loss=2.2890, val_acc=15.8900%, ε=7.3154\n","[DP-SGD] Epoch 18: train_loss=2.2583, val_loss=2.2875, val_acc=15.9100%, ε=7.5469\n","[DP-SGD] Epoch 19: train_loss=2.2635, val_loss=2.2859, val_acc=15.8100%, ε=7.7738\n","[DP-SGD] Epoch 20: train_loss=2.2601, val_loss=2.2840, val_acc=15.6400%, ε=7.9964\n","\n","[DP-SGD]\n","Best val acc: 15.9100%, Test acc: 16.8700%\n"," ε=7.9964\n","Total runtime: 64.4579 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(15.91, 16.87, np.float64(7.9964369216751034))"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["DATASET_NAME = \"CIFAR\"\n","N_CLUSTERS = 500\n","train_loader, val_loader, test_loader = select_dataset_kmedoids(DATASET_NAME, N_CLUSTERS)\n","\n","print(f\"\\nRunning DP-SGD baseline for fixed ({TARGET_EPSILON}, {TARGET_DELTA})-DP\")\n","run_experiment(dataset=DATASET_NAME, use_dp=True, fixed_privacy_budget=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k8NnEXjoHocT","executionInfo":{"status":"ok","timestamp":1765150599743,"user_tz":480,"elapsed":79002,"user":{"displayName":"Madison Sarmiento","userId":"03614282656919606177"}},"outputId":"23c3e1ad-dbce-4f2f-a9f2-da74c0c54bc1"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running DP-SGD baseline for fixed (8, 1e-05)-DP\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n","  warnings.warn(\n","sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"]},{"output_type":"stream","name":"stdout","text":["[DP-SGD] Epoch 1: train_loss=2.3029, val_loss=2.3045, val_acc=8.7100%, ε=1.9909\n","[DP-SGD] Epoch 2: train_loss=2.3055, val_loss=2.3038, val_acc=8.9000%, ε=2.6240\n","[DP-SGD] Epoch 3: train_loss=2.3010, val_loss=2.3029, val_acc=9.1100%, ε=3.1260\n","[DP-SGD] Epoch 4: train_loss=2.2999, val_loss=2.3019, val_acc=9.5100%, ε=3.5598\n","[DP-SGD] Epoch 5: train_loss=2.3006, val_loss=2.3008, val_acc=9.5900%, ε=3.9499\n","[DP-SGD] Epoch 6: train_loss=2.2954, val_loss=2.2997, val_acc=9.5900%, ε=4.3089\n","[DP-SGD] Epoch 7: train_loss=2.2926, val_loss=2.2987, val_acc=9.5900%, ε=4.6443\n","[DP-SGD] Epoch 8: train_loss=2.2938, val_loss=2.2976, val_acc=9.7100%, ε=4.9609\n","[DP-SGD] Epoch 9: train_loss=2.2878, val_loss=2.2965, val_acc=9.8600%, ε=5.2620\n","[DP-SGD] Epoch 10: train_loss=2.2820, val_loss=2.2954, val_acc=10.3700%, ε=5.5503\n","[DP-SGD] Epoch 11: train_loss=2.2816, val_loss=2.2943, val_acc=10.9600%, ε=5.8275\n","[DP-SGD] Epoch 12: train_loss=2.2781, val_loss=2.2929, val_acc=11.5300%, ε=6.0951\n","[DP-SGD] Epoch 13: train_loss=2.2721, val_loss=2.2917, val_acc=11.5500%, ε=6.3544\n","[DP-SGD] Epoch 14: train_loss=2.2727, val_loss=2.2906, val_acc=11.3500%, ε=6.6061\n","[DP-SGD] Epoch 15: train_loss=2.2685, val_loss=2.2893, val_acc=11.4100%, ε=6.8512\n","[DP-SGD] Epoch 16: train_loss=2.2606, val_loss=2.2879, val_acc=11.3200%, ε=7.0902\n","[DP-SGD] Epoch 17: train_loss=2.2621, val_loss=2.2864, val_acc=11.5400%, ε=7.3238\n","[DP-SGD] Epoch 18: train_loss=2.2553, val_loss=2.2846, val_acc=12.0600%, ε=7.5523\n","[DP-SGD] Epoch 19: train_loss=2.2636, val_loss=2.2825, val_acc=12.6700%, ε=7.7762\n","[DP-SGD] Epoch 20: train_loss=2.2465, val_loss=2.2799, val_acc=13.5100%, ε=7.9958\n","\n","[DP-SGD]\n","Best val acc: 13.5100%, Test acc: 14.0300%\n"," ε=7.9958\n","Total runtime: 63.1347 seconds\n"]},{"output_type":"execute_result","data":{"text/plain":["(13.51, 14.03, np.float64(7.995831917202194))"]},"metadata":{},"execution_count":22}]}]}